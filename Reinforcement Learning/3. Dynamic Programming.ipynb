{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Dynamic Programming\n",
    "A discrete dynamic program consists of the following components:\n",
    "\n",
    "* finite set of states $S = \\{0, \\ldots, n-1\\}$;\n",
    "* finite set of available actions $A(s)$ for each state $s\n",
    "\\in S$ with $A = \\bigcup_{s \\in S} A(s) = \\{0, \\ldots, m-1\\}$,\n",
    "where $\\mathit{SA} = \\{(s, a) \\in S \\times A \\mid a \\in A(s)\\}$\n",
    "is the set of feasible state-action pairs;\n",
    "* reward function $r\\colon \\mathit{SA} \\to \\mathbb{R}$, where\n",
    "$r(s, a)$ is the reward when the current state is $s$ and\n",
    "the action chosen is $a$;\n",
    "* transition probability function $q\\colon \\mathit{SA} \\to\n",
    "\\Delta(S)$, where $q(s'|s, a)$ is the probability that the state\n",
    "in the next period is $s'$ when the current state is $s$\n",
    "and the action chosen is $a$; and\n",
    "* discount factor $0 \\leq \\gamma < 1$.\n",
    "\n",
    "#### Policy Function $\\pi$\n",
    "For a policy function $\\pi$, let $r_{\\pi}$ and\n",
    "$P_{\\pi}$ be the reward vector and the transition probability\n",
    "matrix for $\\pi$, which are defined by $r_{\\pi}(s) =\n",
    "r(s, \\pi(s))$ and $P_{\\pi}(s, s') = q(s'|s, \\pi(s))$,\n",
    "respectively. The policy state value function $v_{\\pi}$ for\n",
    "$\\pi$ is defined by\n",
    "\n",
    "\n",
    "\n",
    " $v_{\\pi}(s) = \\sum_{t=0}^{\\infty}\n",
    "                  \\gamma^t (P_{\\pi}^t r_{\\pi})(s)\n",
    "                  \\quad (s \\in S).$\n",
    "\n",
    "The *optimal value function* $v^*$ is the function such that\n",
    "$v^*(s) = \\max_{\\sigma} v_{\\sigma}(s)$ for all $s \\in S$. A\n",
    "policy function $\\sigma^*$ is *optimal* if $v_{\\sigma^*}(s)\n",
    "= v^*(s)$ for all $s \\in S$.\n",
    "\n",
    "The *Bellman equation* is written as\n",
    "\n",
    "\n",
    "\n",
    "$v(s) = \\max_{a \\in A(s)} (r(s, a)\n",
    "         + \\beta \\sum_{s' \\in S} q(s'|s, a) v(s')) \\quad(s \\in S).$\n",
    "\n",
    "The *Bellman operator* $T$ is defined by the right hand side of\n",
    "the Bellman equation:\n",
    "\n",
    "\n",
    "$(T v)(s) = \\max_{a \\in A(s)} (r(s, a)\n",
    "             + \\beta \\sum_{s' \\in S} q(s'|s, a) v(s')) \\quad (s \\in S).$\n",
    "\n",
    "For a policy function $\\sigma$, the operator $T_{\\sigma}$ is\n",
    "defined by\n",
    "\n",
    "\n",
    "$(T_{\\sigma} v)(s) = r(s, \\sigma(s))\n",
    "                      + \\beta \\sum_{s' \\in S} q(s'|s, \\sigma(s)) v(s')\n",
    "                      \\quad (s \\in S),$\n",
    "\n",
    "or $T_{\\sigma} v = r_{\\sigma} + \\beta Q_{\\sigma} v$.\n",
    "\n",
    "The main result of the theory of dynamic programming states that the\n",
    "optimal value function $v^*$ is the unique solution to the Bellman\n",
    "equation, or the unique fixed point of the Bellman operator, and that\n",
    "$\\sigma^*$ is an optimal policy function if and only if it is\n",
    "$v^*$-greedy, i.e., it satisfies $T v^* = T_{\\sigma^*} v^*$.\n",
    "\n",
    "Solution Algorithms\n",
    "-------------------\n",
    "\n",
    "The $DiscreteDP$ class currently implements the following solution\n",
    "algorithms:\n",
    "\n",
    "* value iteration;\n",
    "* policy iteration;\n",
    "* modified policy iteration.\n",
    "\n",
    "Policy iteration computes an exact optimal policy in finitely many\n",
    "iterations, while value iteration and modified policy iteration return\n",
    "an $\\varepsilon$-optimal policy and an\n",
    "$\\varepsilon/2$-approximation of the optimal value function for a\n",
    "prespecified value of $\\varepsilon$.\n",
    "\n",
    "Our implementations of value iteration and modified policy iteration\n",
    "employ the norm-based and span-based termination rules, respectively.\n",
    "\n",
    "* Value iteration is terminated when the condition $\\lVert T v - v\n",
    "\\rVert < [(1 - \\beta) / (2\\beta)] \\varepsilon$ is satisfied.\n",
    "\n",
    "* Modified policy iteration is terminated when the condition\n",
    "$\\mathrm{span}(T v - v) < [(1 - \\beta) / \\beta] \\varepsilon$ is\n",
    "satisfied, where $\\mathrm{span}(z) = \\max(z) - \\min(z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Abortion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In-place dp\n",
    "-  Sweeping\n",
    "    - state which changes most\n",
    "- Real-time\n",
    "\n",
    "## sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantecon.markov import DiscreteDP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "R = [[5, 10], [-1, -float('inf')]]\n",
    "Q = [[(0.5, 0.5), (0, 1)], [(0, 1), (0.5, 0.5)]]\n",
    "beta = 0.95\n",
    "ddp = DiscreteDP(R, Q, beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pprint\n",
    "pp=pprint.PrettyPrinter(indent=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "array([[  5.,  10.],\n",
      "       [ -1., -inf]])\n",
      "array([[[0.5, 0.5],\n",
      "        [0. , 1. ]],\n",
      "\n",
      "       [[0. , 1. ],\n",
      "        [0.5, 0.5]]])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "R_array=np.asarray(R)\n",
    "Q_array=np.asarray(Q)\n",
    "pp.pprint(R_array)\n",
    "pp.pprint(Q_array)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Policy iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ddp.solve(method='policy_iteration', v_init=[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n",
      "[ -8.57142857 -20.        ]\n"
     ]
    }
   ],
   "source": [
    "print(res.sigma)\n",
    "print(res.v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Value iteration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_v = ddp.solve(method='value_iteration', v_init=[0, 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0]\n",
      "[ -8.570939   -19.99951043]\n"
     ]
    }
   ],
   "source": [
    "print(res_v.sigma)\n",
    "print(res_v.v)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "值迭代策略利用了上节中公式（2）\n",
    "\n",
    "    内循环的实现有两种策略：\n",
    "\n",
    "    1、 同步迭代法\n",
    "\n",
    "    拿初始化后的第一次迭代来说吧，初始状态所有的V(s)都为0。然后对所有的s都计算新的V(s)=R(s)+0=R(s)。在计算每一个状态时，得到新的V(s)后，先存下来，不立即更新。待所有的s的新值V(s)都计算完毕后，再统一更新。\n",
    "\n",
    "这样，第一次迭代后，V(s)=R(s)。\n",
    "\n",
    "    2、 异步迭代法\n",
    "\n",
    "    与同步迭代对应的就是异步迭代了，对每一个状态s，得到新的V(s)后，不存储，直接更新。这样，第一次迭代后，大部分V(s)>R(s)。\n",
    "\n",
    "    不管使用这两种的哪一种，最终V(s)会收敛到V*(s)。知道了V*后，我们再使用公式（3）来求出相应的最优策略clip_image069[4]，当然clip_image069[5]可以在求V*的过程中求出。\n",
    "\n",
    "    * 策略迭代法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
