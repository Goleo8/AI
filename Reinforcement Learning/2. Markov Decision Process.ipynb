{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# finite MDP\n",
    "\n",
    "<img src='../pics/markov categories.png'>\n",
    "\n",
    "This is the basis of reinforcement learning, as it can be described with mathematics formula and is tactable.   \n",
    "#### Difference with bandit problem   \n",
    "In bandit problem, try to predict $q(a)$, while in markov decision process $q(s,a)$ and $\\nu_\\ast(s)$ is predicted   \n",
    "\n",
    "#### Some basic concept   \n",
    "\n",
    "1. Agent: Learner and decision maker is called Agent\n",
    "1. Environment: Everything outside the agent and interact with the agent is called `Environment`\n",
    "1. State: at each step of the agent receives some representation of the Environment's `state`\n",
    "1. Action: the action which agent takes to react to environment\n",
    "1. Reward: based on the state of the agent and the action taken by the agent, a reward is gaven to the agent\n",
    "\n",
    "\"one signal to represent the choices made by the agent (the actions), one signal to represent the basis on which the choices are made (the states), and one signal to define the agent’s goal (the rewards).\"[1]\n",
    "<img src='../pics/agent-environment-interaction.png'>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A markov process can be described as the following components:\n",
    "\n",
    "1. States set: s\n",
    "1. Actions set: a\n",
    "1. Rewards: r\n",
    "1. States transfer possibility matrix: $P_{sa}$\n",
    "1. Discount coefficient: $\\gamma$\n",
    "\n",
    "The policy function $\\pi$ define which action **a** should be taken in state **s**. In order to compare the performance of policy $\\pi$. The accumulated reward function is bring in.   \n",
    "\n",
    "$V^\\pi(s)=R(s_0)+\\gamma R(s_1)+...+\\gamma^nS(s_n)=R(s_0)+\\gamma V^\\pi(s_1)$\n",
    "\n",
    "\n",
    "This is a markov process, which means:   \n",
    "* $p(s',r\\mid s,a)\\doteq Pr\\{S_t=s',R_t=r\\mid S_{t-1}=s,A_{t-1}=a\\}$     \n",
    "\n",
    "The dot over the equal sign means this is a definition   \n",
    "* $p(s'\\mid s,a)\\doteq Pr\\{S_t=s'\\mid S_{t-1}=s,A_{t-1}=a\\} = \\sum\\limits_{r\\in R}p(s',r\\mid s,a)$\n",
    "\n",
    "Expected rewards of state $s$ with action $a$   \n",
    "* $r(s,a) \\doteq E\\{R_t \\mid S_{t-1}=s, A_{t-1}=a\\} = \\sum\\limits_{r\\in R}r \\sum\\limits_{s'\\in S}p(s',r \\mid s,a) $\n",
    "\n",
    "The expected rewards of state-action-next_state is a triple tuple\n",
    "* $r(s,a,s') \\doteq E\\{R_t \\mid S_{t-1}=s, A_{t-1}=a,S_t=s'\\} =\\sum\\limits_{r \\in R} r\\frac{p(s',r \\mid s,a)}{ p(s'|s,a)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Exercise 3.1*** Devise three example tasks of your own that fit into the MDP framework, identifying for each its states, actions, and rewards. Make the three examples as different from each other as possible. The framework is abstract and flexible and can be applied in many different ways. Stretch its limits in some way in at least one of your examples. 􏰄\n",
    "\n",
    "Answer: \n",
    "1. Manage the aws emr cluster. The state is the how many cpu cores and memories are running. Instead of using instance counts is different kind of instances have different cpu cores. The action is whether we need start more cpus or shut down some cpus. The reward is whether the budget has been saved.\n",
    "1. Portfolio management. The state is how the position hold and the action is long or short some targets and rewards are whether the deposit is increased or decreased. The limit is the $r(s,a)$ is hard to predict.\n",
    "1. Traffic management. The state is how many cars in each intersections. Action is red or green for traffic light. The rewards is the overall $\\sum t_{waiting}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___Exercise 3.2___ Is the MDP framework adequate to usefully represent all goal-directed learning tasks? Can you think of any clear exceptions? 􏰄\n",
    "\n",
    "Answer:\n",
    "* Like the portfolio management, it does not only depend on current state, but depends on previous states"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\heartsuit$\n",
    "***Exercise 3.3*** Consider the problem of driving. You could define the actions in terms of the accelerator, steering wheel, and brake, that is, where your body meets the machine. Or you could define them farther out—say, where the rubber meets the road, considering your actions to be tire torques. Or you could define them farther in—say, where your brain meets your body, the actions being muscle twitches to control your limbs. Or you could go to a really high level and say that your actions are your choices of where to drive. What is the right level, the right place to draw the line between agent and environment? On what basis is one location of the line to be preferred over another? Is there any fundamental reason for preferring one location over another, or is it a free choice? 􏰄\n",
    "***Exercise 3.4*** If the current state is St, and actions are selected according to stochastic policy π, then what is the expectation of Rt+1 in terms of π and the four-argument function p (3.2)? 􏰄\n",
    "***Exercise 3.5*** Give a table analogous to that in Example 3.3, but for p(s′,r|s,a). It should have columns for s, a, s′, r, and p(s′,r|s,a), and a row for every 4-tuple for which p(s′,r|s,a) > 0."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some concepts   \n",
    "\n",
    "#### Episode task\n",
    "* Episode   \n",
    "Like playing game, each round is an episode\n",
    "* Terminal state   \n",
    "Each episode has a terminal state and then the how many rewards get will be calculated\n",
    "\n",
    "$G_t= R_{t+1}+R_{t+2}+...+R_T$\n",
    "#### Continuing task\n",
    "The task is ongoing and will never be terminated\n",
    "* discounting factor   \n",
    "As the `reward` will become infinite with the time going, then a discounting factor is needed.\n",
    "$G_t \\doteq \\sum\\limits_{k=0}^\\infty \\gamma R_{t+k+1}$, where $0\\leq \\gamma \\leq 1$   \n",
    "\n",
    "$G_t \\doteq R_{t+1}+\\gamma G_{t+1}$\n",
    "\n",
    "### Unified notion for Episodic and Continuing Tasks\n",
    "\n",
    "The episodic task can be convert to continuing task. When the terminal state be taken as a loop with 0 reward, then this process can be taken as a continuing tasks\n",
    "<img src='../pics/episodic-continuing.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\spadesuit$\n",
    "Exercise 3.6 The equations in Section 3.1 are for the continuing case and need to be modified (very slightly) to apply to episodic tasks. Show that you know the modifications needed by giving the modified version of (3.3). 􏰄\n",
    "Exercise 3.7 Suppose you treated pole-balancing as an episodic task but also used discounting, with all rewards zero except for −1 upon failure. What then would the return be at each time? How does this return differ from that in the discounted, continuing formulation of this task? 􏰄\n",
    "Exercise 3.8 Imagine that you are designing a robot to run a maze. You decide to give it a reward of +1 for escaping from the maze and a reward of zero at all other times. The task seems to break down naturally into episodes—the successive runs through the maze—so you decide to treat it as an episodic task, where the goal is to maximize expected total reward (3.7). After running the learning agent for a while, you find that it is showing no improvement in escaping from the maze. What is going wrong? Have you effectively communicated to the agent what you want it to achieve? 􏰄\n",
    "Exercise 3.9 Suppose γ = 0.5 and the following sequence of rewards is received R1 = −1, R2 = 2, R3 =6,R4 =3,andR5 =2,withT =5. WhatareG0,G1,...,G5? Hint: Workbackwards. 􏰄\n",
    "Exercise 3.10 Suppose γ = 0.9 and the reward sequence is R1 = 2 followed by an infinite sequence of 7s. What are G1 and G0? 􏰄\n",
    "Exercise 3.11 Prove (3.10). 􏰄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Policies and Value Function\n",
    "\n",
    "- Key Concept\n",
    "    - `Policy`: policy is a mapping from the state to possibility of selecting each possible action. If the agent take action **a** at time **t**, then $\\pi(a\\mid s)$ is a possibility that $A_t=a$ if $S_t=s$\n",
    "    - `value`: a value of a state under a policy $\\pi$ is the expected reward a state can get if following policy $\\pi$. \n",
    "    - $S,A,P,\\gamma,R$\n",
    "    - $V^\\pi(s)$\n",
    "    - $Q^\\pi(a,s)$\n",
    "- Parameter estimation\n",
    "    - $P_{s,a}(s') =\n",
    "        \\frac{count(s',a \\mid s)}{count(a \\mid s)}$\n",
    "    - $P_{s,a}(s') =\n",
    "        \\frac{1}{|s|}$   \n",
    "\n",
    "### Valution function\n",
    "\n",
    "valution function\n",
    "* $\\mathcal{v}_\\pi(s) \\doteq \\mathbb{E}[G_t \\mid S_t=s] = \\mathbb{E}[\\sum\\limits_{k=0}^\\infty\\gamma^kR_{t+k+1} \\mid S_t=s]$, for all $\\mathcal{s} \\in \\mathcal{S}$ \n",
    "\n",
    "action-value function\n",
    "* $\\mathcal{q}_\\pi(s,a) \\doteq \\mathbb{E}[G_t \\mid S_t=s,A_t=a] = \\mathbb{E}[\\sum\\limits_{k=0}^\\infty\\gamma^kR_{t+k+1} \\mid S_t=s,A_t=a]$, for all $\\mathcal{s} \\in \\mathcal{S} , \\mathcal{a} \\in \\mathcal{A}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\spadesuit$\n",
    "Exercise 3.12 The Bellman equation (3.14) must hold for each state for the value function vπ shown in Figure 3.2 (right) of Example 3.5. Show numerically that this equation holds for the center state, valued at +0.7, with respect to its four neighboring states, valued at +2.3, +0.4, −0.4, and +0.7. (These\n",
    "\n",
    "numbers are accurate only to one decimal place.)\n",
    "Exercise 3.13 What is the Bellman equation for action values, that is, for qπ? It must give the action value qπ(s,a) in terms of the action values, qπ(s′,a′), of possible succes- sors to the state–action pair (s, a). Hint: the backup diagram to the right corresponds to this equation. Show the sequence of equations analogous to (3.14), but for action values. 􏰄\n",
    "s,a pr\n",
    "         Exercise 3.14 In the gridworld example, rewards are positive for goals, negative for\n",
    "running into the edge of the world, and zero the rest of the time. Are the signs of these rewards important, or only the intervals between them? Prove, using (3.8), that adding a constant c to all the rewards adds a constant, vc, to the values of all states, and thus does not affect the relative values of any states under any policies. What is vc in terms of c and γ? 􏰄\n",
    "Exercise 3.15 Now consider adding a constant running. Would this have any effect, or would above? Why or why not? Give an example.\n",
    "\n",
    "\n",
    "Exercise 3.16 The value of a state depends on the values of the actions possible in that state and on how likely each action is to be taken under the current policy. We can think of this in terms of a small backup diagram rooted at the state and considering each possible action:\n",
    "Give the equation corresponding to this intuition and diagram for the value at the root node, vπ(s), in terms of the value at the expected leaf node, qπ(s,a), given St = s. This equation should include an expectation conditioned on following the policy, π. Then give a second equation in which the expected value is written out explicitly in terms of π(a|s) such that no expected value notation appears in the equation. 􏰄\n",
    "Exercise 3.17 The value of an action, qπ(s,a), depends on the expected next reward and the expected sum of the remaining rewards. Again we can think of this in terms of a small backup diagram, this one rooted at an action (state–action pair) and branching to the possible next states:\n",
    "expected s, a q⇡ (s, a)\n",
    "  taken with probability ⇡(a|s)\n",
    "s\n",
    "a1 a2 a3\n",
    "v⇡ (s) q⇡ (s, a)\n",
    "           rewards\n",
    "r1 r2 r3\n",
    "s01 s02 s03\n",
    " v⇡(s0)\n",
    " Give the equation corresponding to this intuition and diagram for the action value, qπ(s,a), in terms of the expected next reward, Rt+1, and the expected next state value, vπ(St+1), given that St =s and At = a. This equation should include an expectation but not one conditioned on following the policy. Then give a second equation, writing out the expected value explicitly in terms of p(s′,r|s,a) defined by (3.2), such that no expected value notation appears in the equation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimal Policies and Optimal Value Functions\n",
    "\n",
    "#### How to define a policy is better than another\n",
    "\n",
    "If the value function of each state under policy $\\pi(a)$ is $\\geq$ the value function under $\\pi'(a)$, then policy $\\pi(a)$ is better.   \n",
    "We denote the optimal policies set with $\\pi_{\\ast}$. They share the same state value function.\n",
    "$\\nu_{\\ast}(s)\\doteq \\max\\limits_{\\pi}\\nu_{\\pi}(s)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bellman Equation\n",
    "\n",
    "### Bellman Equation for Value Function\n",
    "\n",
    "\\begin{equation}\n",
    "\\mathcal{v}_{\\ast}(s) = \\max\\limits_{\\mathcal{a}\\in\\mathcal{A}(s)}q_{\\pi_\\ast}(s,a)\\\\\n",
    "= \\max_a\\mathbb{E}_{\\pi_{\\ast}}[G_t \\mid S_t=t,A_t=a] \\\\\n",
    "= \\max_a\\mathbb{E}_{\\pi_{\\ast}}[R_t+\\gamma G_{t+1} \\mid S_t=t,A_t=a] \\\\\n",
    "= \\max_a\\mathbb{E}_{\\pi_{\\ast}}[R_t+\\gamma \\mathcal{v}_\\ast(S_{t+1}) \\mid S_t=t,A_t=a] \\\\\n",
    "= \\max_a\\sum\\limits_{s',r}p(s',r \\mid s,a)[r+\\gamma \\mathcal{v}_\\ast(s')].\n",
    "\\end{equation}\n",
    "\n",
    "### Bellman Equation for action-value function\n",
    "\\begin{equation}\n",
    "q_\\ast(s,a)=\\mathbb{E}[R_{t+1}+\\gamma \\max_{a'}q_\\ast(S_{t+1},a') \\mid S_t=s,A_t=a] \\\\\n",
    "= \\sum_{s',r}p(s',r \\mid s,a)[r+\\gamma \\max_{a'}q_\\ast(s',a')]\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## $\\spadesuit$\n",
    "Exercise 3.18 Draw or describe the optimal state-value function for the golf example. 􏰄 Exercise 3.19 Draw or describe the contours of the optimal action-value function for putting, q∗(s, putter),\n",
    "for the golf example. 􏰄 Exercise 3.20 Give the Bellman equation for q∗ for the recycling robot. 􏰄\n",
    "Exercise 3.21 Figure 3.5 gives the optimal value of the best state of the gridworld as 24.4, to one decimal place. Use your knowledge of the optimal policy and (3.8) to express this value symbolically, and then to compute it to three decimal places. 􏰄\n",
    "\n",
    "Exercise 3.22 Consider the continuing MDP shown on to the right. The only decision to be made is that in the top state, where two actions are available, left and right. The numbers show the rewards that are received deterministically after each action. There are exactly two deterministic policies, πleft and πright. Whatpolicyisoptimalifγ=0? Ifγ=0.9? Ifγ=0.5? 􏰄\n",
    "Exercise 3.23 Give an equation for v∗ in terms of q∗. 􏰄\n",
    "Exercise 3.24 Give an equation for q∗ in terms of v∗ and the world’s dynamics, p(s′,r|s,a). 􏰄 Exercise 3.25 Give an equation for π∗ in terms of q∗. 􏰄 Exercise 3.26 Give an equation for π∗ in terms of v∗ and the world’s dynamics, p(s′,r|s,a). 􏰄"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Crossentropy method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
