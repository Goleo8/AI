{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Dynamic Programming\n",
    "A discrete dynamic program consists of the following components:\n",
    "\n",
    "* finite set of states $S = \\{0, \\ldots, n-1\\}$;\n",
    "* finite set of available actions $A(s)$ for each state $s\n",
    "\\in S$ with $A = \\bigcup_{s \\in S} A(s) = \\{0, \\ldots, m-1\\}$,\n",
    "where $\\mathit{SA} = \\{(s, a) \\in S \\times A \\mid a \\in A(s)\\}$\n",
    "is the set of feasible state-action pairs;\n",
    "* reward function $r\\colon \\mathit{SA} \\to \\mathbb{R}$, where\n",
    "$r(s, a)$ is the reward when the current state is $s$ and\n",
    "the action chosen is $a$;\n",
    "* transition probability function $q\\colon \\mathit{SA} \\to\n",
    "\\Delta(S)$, where $q(s'|s, a)$ is the probability that the state\n",
    "in the next period is $s'$ when the current state is $s$\n",
    "and the action chosen is $a$; and\n",
    "* discount factor $0 \\leq \\beta < 1$.\n",
    "\n",
    "For a policy function $\\sigma$, let $r_{\\sigma}$ and\n",
    "$Q_{\\sigma}$ be the reward vector and the transition probability\n",
    "matrix for $\\sigma$, which are defined by $r_{\\sigma}(s) =\n",
    "r(s, \\sigma(s))$ and $Q_{\\sigma}(s, s') = q(s'|s, \\sigma(s))$,\n",
    "respectively. The policy value function $v_{\\sigma}$ for\n",
    "$\\sigma$ is defined by\n",
    "\n",
    "\n",
    "\n",
    " $v_{\\sigma}(s) = \\sum_{t=0}^{\\infty}\n",
    "                  \\beta^t (Q_{\\sigma}^t r_{\\sigma})(s)\n",
    "                  \\quad (s \\in S).$\n",
    "\n",
    "The *optimal value function* $v^*$ is the function such that\n",
    "$v^*(s) = \\max_{\\sigma} v_{\\sigma}(s)$ for all $s \\in S$. A\n",
    "policy function $\\sigma^*$ is *optimal* if $v_{\\sigma^*}(s)\n",
    "= v^*(s)$ for all $s \\in S$.\n",
    "\n",
    "The *Bellman equation* is written as\n",
    "\n",
    "\n",
    "\n",
    "$v(s) = \\max_{a \\in A(s)} (r(s, a)\n",
    "         + \\beta \\sum_{s' \\in S} q(s'|s, a) v(s')) \\quad(s \\in S).$\n",
    "\n",
    "The *Bellman operator* $T$ is defined by the right hand side of\n",
    "the Bellman equation:\n",
    "\n",
    "\n",
    "$(T v)(s) = \\max_{a \\in A(s)} (r(s, a)\n",
    "             + \\beta \\sum_{s' \\in S} q(s'|s, a) v(s')) \\quad (s \\in S).$\n",
    "\n",
    "For a policy function $\\sigma$, the operator $T_{\\sigma}$ is\n",
    "defined by\n",
    "\n",
    "\n",
    "$(T_{\\sigma} v)(s) = r(s, \\sigma(s))\n",
    "                      + \\beta \\sum_{s' \\in S} q(s'|s, \\sigma(s)) v(s')\n",
    "                      \\quad (s \\in S),$\n",
    "\n",
    "or $T_{\\sigma} v = r_{\\sigma} + \\beta Q_{\\sigma} v$.\n",
    "\n",
    "The main result of the theory of dynamic programming states that the\n",
    "optimal value function $v^*$ is the unique solution to the Bellman\n",
    "equation, or the unique fixed point of the Bellman operator, and that\n",
    "$\\sigma^*$ is an optimal policy function if and only if it is\n",
    "$v^*$-greedy, i.e., it satisfies $T v^* = T_{\\sigma^*} v^*$.\n",
    "\n",
    "Solution Algorithms\n",
    "-------------------\n",
    "\n",
    "The $DiscreteDP$ class currently implements the following solution\n",
    "algorithms:\n",
    "\n",
    "* value iteration;\n",
    "* policy iteration;\n",
    "* modified policy iteration.\n",
    "\n",
    "Policy iteration computes an exact optimal policy in finitely many\n",
    "iterations, while value iteration and modified policy iteration return\n",
    "an $\\varepsilon$-optimal policy and an\n",
    "$\\varepsilon/2$-approximation of the optimal value function for a\n",
    "prespecified value of $\\varepsilon$.\n",
    "\n",
    "Our implementations of value iteration and modified policy iteration\n",
    "employ the norm-based and span-based termination rules, respectively.\n",
    "\n",
    "* Value iteration is terminated when the condition $\\lVert T v - v\n",
    "\\rVert < [(1 - \\beta) / (2\\beta)] \\varepsilon$ is satisfied.\n",
    "\n",
    "* Modified policy iteration is terminated when the condition\n",
    "$\\mathrm{span}(T v - v) < [(1 - \\beta) / \\beta] \\varepsilon$ is\n",
    "satisfied, where $\\mathrm{span}(z) = \\max(z) - \\min(z)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Early Abortion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- In-place dp\n",
    "-  Sweeping\n",
    "    - state which changes most\n",
    "- Real-time\n",
    "\n",
    "## sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from quantecon.markov import DiscreteDP"
   ]
  },
  {
   "cell_type": "code",
<<<<<<< HEAD
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
=======
   "execution_count": 5,
   "metadata": {},
>>>>>>> 4834b7d22472ac261ae39971f6cdff2677a02c7d
   "outputs": [],
   "source": [
    "R = [[5, 10], [-1, -float('inf')]]\n",
    "Q = [[(0.5, 0.5), (0, 1)], [(0, 1), (0.5, 0.5)]]\n",
    "beta = 0.95\n",
    "ddp = DiscreteDP(R, Q, beta)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
