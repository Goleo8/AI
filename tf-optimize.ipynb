{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.7.1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.datasets import fetch_california_housing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from functools import partial\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=fetch_california_housing()\n",
    "X,y=data.data,data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=.3,random_state=42)\n",
    "sc=StandardScaler()\n",
    "X_train_s=sc.fit_transform(X_train)\n",
    "X_test_s=sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch:0,acc:0.916267\n",
      "epoch:1,acc:0.907717\n",
      "epoch:2,acc:0.915508\n",
      "epoch:3,acc:0.912587\n",
      "epoch:4,acc:0.906437\n",
      "epoch:5,acc:0.905772\n",
      "epoch:6,acc:0.906584\n",
      "epoch:7,acc:0.906846\n",
      "epoch:8,acc:0.906934\n",
      "epoch:9,acc:0.907237\n"
     ]
    }
   ],
   "source": [
    "initial_learning_rate=0.01\n",
    "n_decay_step=1000\n",
    "n_decay_rate=1/10\n",
    "momentum=0.9\n",
    "n_h1=10\n",
    "n_h2=10\n",
    "n_h3=1\n",
    "n_batch=64\n",
    "n_loop=X_train_s.shape[0]//n_batch\n",
    "n_epoch=10\n",
    "scale=0.001\n",
    "dropout_rate=.5\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "def shuff_data(X,y):\n",
    "    index = np.random.permutation(X.shape[0])\n",
    "    return X[index],y[index]\n",
    "\n",
    "def get_batch(X,y,batch_index,batch_size):\n",
    "    return X[batch_index*batch_size:(batch_index+1)*batch_size],y[batch_index*batch_size:(batch_index+1)*batch_size]\n",
    "\n",
    "initlizer = tf.initializers.variance_scaling(mode='fan_avg')\n",
    "partial_layer=partial(tf.layers.dense,activation=tf.nn.elu,kernel_initializer=initlizer,kernel_regularizer=tf.contrib.layers.l1_regularizer(scale))\n",
    "    \n",
    "\n",
    "\n",
    "X=tf.placeholder(tf.float32,shape=(None,X_train_s.shape[1]),name='X')\n",
    "y=tf.placeholder(tf.float32,shape=(None,),name='y')\n",
    "\n",
    "training=False\n",
    "with tf.name_scope('dnn'):\n",
    "    # h1=tf.layers.dense(X,n_h1,activation=tf.nn.elu,kernel_initializer=initlizer,name='h1')\n",
    "    # h2=tf.layers.dense(h1,n_h2,activation=tf.nn.elu,kernel_initializer=initlizer,name='h2')\n",
    "    # h3=tf.layers.dense(h2,n_h3,name='h3')\n",
    "\n",
    "    X_drop=tf.layers.dropout(X,rate=dropout_rate,training=training)\n",
    "    h1=partial_layer(X_drop,n_h1,name='h1')\n",
    "    h1_drop=tf.layers.dropout(h1,rate=dropout_rate,training=training)\n",
    "    h2=partial_layer(h1,n_h2,name='h2')\n",
    "    h2_drop=tf.layers.dropout(h2,rate=dropout_rate,training=training)\n",
    "    h3=partial_layer(h2,n_h3,activation=None,name='h3')\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    err=y-h3\n",
    "    base_loss = tf.reduce_mean(tf.square(err),name='base_loss')\n",
    "    regularizaiton_loss=tf.get_collection(tf.GraphKeys.REGULARIZATION_LOSSES)\n",
    "    loss=tf.add_n([base_loss]+regularizaiton_loss,name='loss')\n",
    "    global_step=tf.Variable(0,trainable=False,name=\"global_step\")\n",
    "    learning_rate = tf.train.exponential_decay(initial_learning_rate,global_step,n_decay_step,n_decay_rate)\n",
    "\n",
    "    training_obj=tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=momentum).minimize(loss,global_step=global_step)\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    acc=tf.reduce_mean(tf.abs(err),name='acc')\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    \n",
    "    for e in range(n_epoch):\n",
    "        X_tmp,y_tmp = shuff_data(X_train_s,y_train)\n",
    "        for l in range(n_loop):\n",
    "            X_batch,y_batch=get_batch(X_tmp,y_tmp,l,n_batch)\n",
    "            sess.run(training_obj,feed_dict={X:X_batch,y:y_batch})\n",
    "        acc_res=acc.eval(feed_dict={X:X_test_s,y:y_test})\n",
    "        print(\"epoch:%d,acc:%f\"%(e,acc_res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
