{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[None, None, None, None, None, <tf.Tensor 'gradients_2/Mul_3_grad/Reshape_1:0' shape=() dtype=float32>]\n",
      "None\n",
      "[2.0, 1.4]\n"
     ]
    }
   ],
   "source": [
    "w1 = tf.Variable(2.0)\n",
    "w2 = tf.Variable(2.0)\n",
    "a = tf.multiply(w1, 3.0)\n",
    "a_stoped = tf.stop_gradient(a)\n",
    "\n",
    "# b=w1*3.0*w2\n",
    "b = tf.multiply(a_stoped, w2)\n",
    "\n",
    "opt = tf.train.GradientDescentOptimizer(0.1)\n",
    "\n",
    "gradients = tf.gradients(b, xs=tf.trainable_variables())\n",
    "\n",
    "# tf.summary.histogram(gradients[0].name, gradients[0])# 这里会报错，因为gradients[0]是None\n",
    "#其它地方都会运行正常，无论是梯度的计算还是变量的更新。总觉着tensorflow这么设计有点不好，\n",
    "#不如改成流过去的梯度为0\n",
    "train_op = opt.apply_gradients(zip(gradients, tf.trainable_variables()))\n",
    "\n",
    "print(gradients)\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(sess.run(train_op))\n",
    "    print(sess.run([w1, w2]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.0, 1.0]\n"
     ]
    }
   ],
   "source": [
    "a = tf.Variable(1.0)\n",
    "b = tf.Variable(1.0)\n",
    "\n",
    "c = tf.add(a, b)\n",
    "\n",
    "c_stoped = tf.stop_gradient(c)\n",
    "\n",
    "d = tf.add(a, b)\n",
    "\n",
    "e = tf.add(c_stoped, d)\n",
    "\n",
    "gradients = tf.gradients(e, xs=[a, b])\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print(sess.run(gradients))\n",
    "#输出 [1.0, 1.0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "@doug's answer has worked for me. There's one additional rule of thumb that helps for supervised learning problems. The upper bound on the number of hidden neurons that won't result in over-fitting is:\n",
    "\n",
    "$$N_h = \\frac{N_s} {(\\alpha * (N_i + N_o))}$$\n",
    "\n",
    "$N_i$ = number of input neurons. $N_o$ = number of output neurons. $N_s$ = number of samples in training data set. $\\alpha$ = an arbitrary scaling factor usually 2-10.\n",
    "Others recommend setting $alpha$ to a value between 5 and 10, but I find a value of 2 will often work without overfitting. As explained by this excellent NN Design text, you want to limit the number of free parameters in your model (its degree or number of nonzero weights) to a small portion of the degrees of freedom in your data. The degrees of freedom in your data is the number samples * degrees of freedom (dimensions) in each sample or $N_s * (N_i + N_o)$ (assuming they're all independent). So $\\alpha$ is a way to indicate how general you want your model to be, or how much you want to prevent overfitting.\n",
    "\n",
    "For an automated procedure you'd start with an alpha of 2 (twice as many degrees of freedom in your training data as your model) and work your way up to 10 if the error for training data is significantly smaller than for the cross-validation data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
