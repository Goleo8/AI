{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "import re\n",
    "import numpy as np\n",
    "from nltk import ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_model = gensim.models.Word2Vec.load('full_grams_sg_300_twitter.mdl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# =========================\n",
    "# ==== Helper Methods =====\n",
    "\n",
    "# Clean/Normalize Arabic Text\n",
    "def clean_str(text):\n",
    "    search = [\"أ\",\"إ\",\"آ\",\"ة\",\"_\",\"-\",\"/\",\".\",\"،\",\" و \",\" يا \",'\"',\"ـ\",\"'\",\"ى\",\"\\\\\",'\\n', '\\t','&quot;','?','؟','!']\n",
    "    replace = [\"ا\",\"ا\",\"ا\",\"ه\",\" \",\" \",\"\",\"\",\"\",\" و\",\" يا\",\"\",\"\",\"\",\"ي\",\"\",' ', ' ',' ',' ? ',' ؟ ',' ! ']\n",
    "    \n",
    "    #remove tashkeel\n",
    "    p_tashkeel = re.compile(r'[\\u0617-\\u061A\\u064B-\\u0652]')\n",
    "    text = re.sub(p_tashkeel,\"\", text)\n",
    "    \n",
    "    #remove longation\n",
    "    p_longation = re.compile(r'(.)\\1+')\n",
    "    subst = r\"\\1\\1\"\n",
    "    text = re.sub(p_longation, subst, text)\n",
    "    \n",
    "    text = text.replace('وو', 'و')\n",
    "    text = text.replace('يي', 'ي')\n",
    "    text = text.replace('اا', 'ا')\n",
    "    \n",
    "    for i in range(0, len(search)):\n",
    "        text = text.replace(search[i], replace[i])\n",
    "    \n",
    "    #trim    \n",
    "    text = text.strip()\n",
    "\n",
    "    return text\n",
    "\n",
    "def get_vec(n_model,dim, token):\n",
    "    vec = np.zeros(dim)\n",
    "    is_vec = False\n",
    "    if token not in n_model.wv:\n",
    "        _count = 0\n",
    "        is_vec = True\n",
    "        for w in token.split(\"_\"):\n",
    "            if w in n_model.wv:\n",
    "                _count += 1\n",
    "                vec += n_model.wv[w]\n",
    "        if _count > 0:\n",
    "            vec = vec / _count\n",
    "    else:\n",
    "        vec = n_model.wv[token]\n",
    "    return vec\n",
    "\n",
    "def calc_vec(pos_tokens, neg_tokens, n_model, dim):\n",
    "    vec = np.zeros(dim)\n",
    "    for p in pos_tokens:\n",
    "        vec += get_vec(n_model,dim,p)\n",
    "    for n in neg_tokens:\n",
    "        vec -= get_vec(n_model,dim,n)\n",
    "    \n",
    "    return vec   \n",
    "\n",
    "## -- Retrieve all ngrams for a text in between a specific range\n",
    "def get_all_ngrams(text, nrange=3):\n",
    "    text = re.sub(r'[\\,\\.\\;\\(\\)\\[\\]\\_\\+\\#\\@\\!\\?\\؟\\^]', ' ', text)\n",
    "    tokens = [token for token in text.split(\" \") if token.strip() != \"\"]\n",
    "    ngs = []\n",
    "    for n in range(2,nrange+1):\n",
    "        ngs += [ng for ng in ngrams(tokens, n)]\n",
    "    return [\"_\".join(ng) for ng in ngs if len(ng)>0 ]\n",
    "\n",
    "## -- Retrieve all ngrams for a text in a specific n\n",
    "def get_ngrams(text, n=2):\n",
    "    text = re.sub(r'[\\,\\.\\;\\(\\)\\[\\]\\_\\+\\#\\@\\!\\?\\؟\\^]', ' ', text)\n",
    "    tokens = [token for token in text.split(\" \") if token.strip() != \"\"]\n",
    "    ngs = [ng for ng in ngrams(tokens, n)]\n",
    "    return [\"_\".join(ng) for ng in ngs if len(ng)>0 ]\n",
    "\n",
    "## -- filter the existed tokens in a specific model\n",
    "def get_existed_tokens(tokens, n_model):\n",
    "    return [tok for tok in tokens if tok in n_model.wv ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "token = clean_str(u'ابو تريكه').replace(\" \", \"_\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ابوتريكه 0.8353127241134644\n",
      "تريكه 0.742644727230072\n",
      "حازم_امام 0.6797752380371094\n",
      "حسام_حسن 0.6696128845214844\n",
      "شيكابالا 0.6619654893875122\n",
      "عمرو_زكي 0.6597729921340942\n",
      "الزمالك 0.654998779296875\n",
      "باسم_مرسي 0.6479896306991577\n",
      "عماد_متعب 0.6467376947402954\n",
      "وائل_جمعه 0.6452205181121826\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    }
   ],
   "source": [
    "if token in t_model.wv:\n",
    "    most_similar = t_model.wv.most_similar( token, topn=10 )\n",
    "    for term, score in most_similar:\n",
    "        term = clean_str(term).replace(\" \", \"_\")\n",
    "        if term != token:\n",
    "            print(term, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "gensim.models.keyedvectors.Word2VecKeyedVectors"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(t_model.wv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_model.wv.save(\"w2v.kv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "t_model.wv.save_word2vec_format(\"w2v.bin\",binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "￙ﾆ￙ﾈ￘ﾱ￙ﾈ￘ﾲ\n",
      "\n",
      "￘ﾧ￙ﾄ￘ﾳ￙ﾄ￘ﾧ￙ﾅ￙ﾇ￘ﾧ￘ﾯ￙ﾊ\n",
      "\n",
      "￘ﾧ￙ﾄ￘ﾪ￘ﾧ￙ﾊ￙ﾄ￙ﾆ￘ﾯ￙ﾊ￘ﾧ￘ﾪ\n",
      "\n",
      "￘ﾲ￙ﾁ￙ﾊ￘ﾱ\n",
      "\n",
      "￙ﾆ￙ﾆￚﾯ\n",
      "\n",
      "￘ﾷ￘ﾱ￘ﾯ_￘ﾧ￙ﾄ￘ﾨ￘ﾹ￘ﾫ￙ﾇ\n",
      "\n",
      "￯ﾻﾗ￯ﾻﾠ￯ﾻﾮ￯ﾺﾑ￯ﾺﾎ\n",
      "\n",
      "￙ﾆ￙ﾈ￘ﾱ_￘ﾧ￙ﾄ￙ﾂ￙ﾅ￘ﾱ\n",
      "\n",
      "￙ﾈ￘ﾨ￘ﾱ￘ﾧ￘ﾦ￘ﾪ￙ﾃ\n",
      "\n",
      "￙ﾈ￘ﾧ￙ﾈ￘ﾬ￘ﾹ￘ﾪ￙ﾆ￙ﾊ\n",
      "\n",
      "#_￘ﾧ￙ﾂ￘ﾧ￙ﾄ￙ﾇ_￘ﾯ￙ﾈ￙ﾆ￙ﾊ￘ﾳ\n",
      "\n",
      "￘ﾨ￘ﾪ￘ﾵ￘ﾨ￘ﾱ￙ﾊ￙ﾆ\n",
      "\n",
      "￙ﾈ￘ﾨ￘ﾭ￘ﾯ￘ﾯ\n",
      "\n",
      "￘ﾧ￘ﾪ￘ﾯ￙ﾄ￙ﾄ￙ﾊ\n",
      "\n",
      "￙ﾈ￘ﾪ￘ﾨ￘ﾧ\n",
      "\n",
      "￙ﾈ￘ﾧ￘ﾭ￙ﾊ￙ﾊ￘ﾪ￙ﾆ￙ﾊ\n",
      "\n",
      "￘ﾧ￙ﾄ￘ﾨ￘ﾱ￙ﾊ￙ﾅ￙ﾊ￘ﾱ￙ﾄ￙ﾊ￙ﾂ\n",
      "\n",
      "￢ﾖﾪ￯ﾸﾏ￢ﾚﾪ￯ﾸﾏ\n",
      "\n",
      "￙ﾆ￘ﾳ￘ﾨ￘ﾪ￙ﾊ\n",
      "\n",
      "￙ﾂ￘ﾱ￙ﾊ￘ﾨ￙ﾈ￘ﾧ￙ﾅ￘ﾳ￘ﾭ\n",
      "\n",
      "￙ﾈ￘ﾨ￙ﾄ￘ﾧ￙ﾇ￘ﾧ\n",
      "\n",
      "￘ﾧ￙ﾂ￘ﾪ￘ﾭ￘ﾧ￙ﾅ_￘ﾧ￙ﾄ￙ﾅ￙ﾆ￘ﾧ￘ﾲ￙ﾄ\n",
      "\n",
      "￘ﾧ￘ﾧ￙ﾄ￙ﾂ￙ﾊ￘ﾧ￙ﾅ￙ﾇ￙ﾇ\n",
      "\n",
      "￙ﾈ￘ﾧ￘ﾲ￙ﾇ￙ﾂ\n",
      "\n",
      "￘ﾪ￙ﾄ￘ﾭ￙ﾁ￙ﾈ\n",
      "\n",
      "￙ﾊ￘ﾭ￘ﾧ￙ﾃ\n",
      "\n",
      "￘ﾧ￘ﾨ￘ﾧ￙ﾄ￙ﾇ￘ﾧ\n",
      "\n",
      "^￙ﾈ￘ﾧ￙ﾆ￘ﾪ￙ﾊ\n",
      "\n",
      "￘ﾵ￘ﾯ￘ﾱ￙ﾅ￙ﾆ\n",
      "\n",
      "￘ﾳ￙ﾊ￙ﾃ￘ﾳ￘ﾨ\n",
      "\n",
      "￘ﾹ￙ﾅ￘ﾱ_￘ﾹ￘ﾨ￘ﾯ_￘ﾧ￙ﾄ￙ﾃ￘ﾧ￙ﾁ￙ﾊ\n",
      "\n",
      "￙ﾂ￘ﾨ￘ﾧ￘ﾧ￘ﾦ￙ﾄ\n",
      "\n",
      "￙ﾅ￘ﾨ￘ﾱ￘ﾨ￘ﾱ\n",
      "\n",
      "￙ﾆ￘ﾺ￙ﾅ￙ﾇ_￙ﾆ￘ﾴ￙ﾊ￘ﾯ\n",
      "\n",
      "￘ﾧ￙ﾅ￘ﾱ￙ﾊ￙ﾃ￘ﾧ￢ﾀﾎ\n",
      "\n",
      "￘ﾧ￙ﾄ￘ﾬ￙ﾅ￘ﾹ￙ﾇ_￙ﾁ￘ﾧ￙ﾃ￘ﾪ￘ﾨ_￙ﾄ￙ﾆ￘ﾧ\n",
      "\n",
      "￙ﾃ￘ﾧ￙ﾆ￙ﾈ￘ﾧ_￘ﾨ￙ﾇ_￙ﾊ￘ﾳ￘ﾪ￙ﾇ￘ﾲ￘ﾦ￙ﾈ￙ﾆ\n",
      "\n",
      "￘ﾭ￙ﾁ￘ﾸￚﾩ\n",
      "\n",
      "￘ﾧ￙ﾄ￘ﾧ￘ﾳ￘ﾪ￘ﾺ￙ﾁ￘ﾧ￘ﾱ￢ﾙﾡ\n",
      "\n",
      "￘ﾨ￘ﾧ￙ﾄ￘ﾳ￘ﾹ￙ﾈ￯﾿ﾽ\n",
      "\n",
      "￘ﾱ￙ﾃ￘ﾹￛﾁ_￙ﾂ￘ﾯ\n",
      "\n",
      "￙ﾈ￘ﾶ￙ﾊ￘ﾹ￘ﾪ￙ﾇ\n",
      "\n",
      "￘ﾭ￙ﾊ￙ﾆ￰ﾟﾒﾛ\n",
      "\n",
      "￘ﾨ￘ﾪ￙ﾆ￘ﾪ￙ﾁ\n",
      "\n",
      "￘ﾨ￘ﾪ￘ﾹ￙ﾅ￙ﾄ￙ﾊ￙ﾄ￙ﾊ\n",
      "\n",
      "￘ﾨ￘ﾧ￙ﾄ￘ﾸ￙ﾇ￘ﾱ￘ﾧ￙ﾆ\n",
      "\n",
      "￢ﾜﾋ￳ﾾﾌﾴ\n",
      "\n",
      "￘ﾧ￘ﾱ￙ﾈ￙ﾊ￘ﾧ\n",
      "\n",
      "￙ﾅ￘ﾧ￘ﾪ￙ﾁ￘ﾪ￙ﾊ￘ﾴ\n",
      "\n",
      "￙ﾆ￙ﾁ￘ﾳ￙ﾊ_￘ﾷ￘ﾱ￙ﾁ￙ﾇ\n",
      "\n",
      "￙ﾅ￘ﾨ￙ﾂ￙ﾈ￙ﾂ￙ﾇ\n",
      "\n",
      "￘ﾧ￙ﾄ￘ﾺ￘ﾧ￘ﾨ￘ﾧ￘ﾪ\n",
      "\n",
      "￙ﾈ￙ﾊ￘ﾯ￙ﾈ￘ﾨ\n",
      "\n",
      "￘ﾧ￙ﾄ￘ﾨ￙ﾊ￘ﾧ￘ﾲ￙ﾊ￙ﾆ\n",
      "\n",
      "￙ﾄ￙ﾃ￘ﾱ￙ﾇ_￘ﾧ￙ﾄ￙ﾂ￘ﾯ￙ﾅ_￘ﾨ￘ﾧ￙ﾄ￙ﾆ￘ﾧ￘ﾯ￙ﾊ\n",
      "\n",
      "￙ﾊ￘ﾶ￘ﾱ￙ﾈￚﾯ\n",
      "\n",
      "￰ﾟﾑﾌ￰ﾟﾏﾻ\n",
      "\n",
      "￙ﾈ￙ﾃ￘ﾰ￙ﾇ\n",
      "\n",
      "￙ﾈ￘ﾱ￘ﾧ￙ﾅ￙ﾇ\n",
      "\n",
      "￘ﾧ￙ﾄ￙ﾈ￘ﾷ￙ﾆ_￘ﾨ￙ﾄ￘ﾳ\n",
      "\n",
      "￢ﾀﾹ￢ﾘﾺ>\n",
      "\n",
      "￘ﾳ￙ﾃ￘ﾨ￘ﾪ\n",
      "\n",
      "￙ﾅ￘ﾳ￘ﾪ￘ﾺ￘ﾱ￘ﾨ￙ﾆ￙ﾊ\n",
      "\n",
      "￙ﾈ￘ﾪ￙ﾆ￘ﾹ￘ﾧ￙ﾁ\n",
      "\n",
      "￘ﾧ￙ﾄ￙ﾄ￙ﾊ￙ﾂ￙ﾈ\n",
      "\n",
      "￘ﾨ￘ﾧ￘ﾳ￘ﾪ￙ﾂ￘ﾷ￘ﾧ￘ﾨ\n",
      "\n",
      "￘ﾮ￙ﾈ\n",
      "\n",
      "￘ﾨ￘ﾧ￙ﾄ￘ﾧ￘ﾹ￘ﾱ￘ﾧ￙ﾂ\n",
      "\n",
      "￘ﾨ￘ﾪ￘ﾪ￙ﾈ￘ﾳ￘ﾮ\n",
      "\n",
      "￙ﾂ￘ﾳ￙ﾅ￙ﾊ￙ﾈ￙ﾅ￙ﾆ\n",
      "\n",
      "￘ﾧ￙ﾄ￙ﾅ￙ﾆ￙ﾊ￘ﾮ￘ﾱ\n",
      "\n",
      "￙ﾈ￘ﾧ￙ﾃ￘ﾴ￘ﾮ￙ﾊ\n",
      "\n",
      "￙ﾊ￙ﾅ￘ﾯ￘ﾯ_￘ﾹ￙ﾂ￘ﾯ\n",
      "\n",
      "￘ﾧ￘ﾵ￙ﾅ￘ﾯ￙ﾈ\n",
      "\n",
      "￘ﾪ￘ﾯ￙ﾆ￙ﾊ￘ﾳ￙ﾇ￘ﾧ\n",
      "\n",
      "￙ﾄ￘ﾧ￙ﾅ￘ﾨ￙ﾄ￘ﾧ￙ﾃ\n",
      "\n",
      "￢ﾀﾻ\n",
      "\n",
      "￙ﾈ￘ﾧ￘ﾮ￘ﾪ￘ﾱ\n",
      "\n",
      "￘ﾨ￙ﾆ￙ﾂ￘ﾨ￙ﾄ\n",
      "\n",
      "￙ﾅ￘ﾮ￘ﾧ￘ﾨ￘ﾱ￘ﾪ￙ﾊ￙ﾇ\n",
      "\n",
      "￙ﾅ￙ﾆ￘ﾪ￘ﾱ￙ﾃ\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('res/part-00000-cead8de6-cc72-4ab1-b66b-7356e9ef8964-c000.csv','r') as input_file:\n",
    "    l = input_file.readlines()\n",
    "    for i in l:\n",
    "        print(u\"{}\".format(i))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_model = gensim.models.KeyedVectors.load_word2vec_format(\"w2v.bin\",binary=True,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:1: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  if __name__ == '__main__':\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/ipykernel/__main__.py:2: DeprecationWarning: Call to deprecated `wv` (Attribute will be removed in 4.0.0, use self instead).\n",
      "  from ipykernel import kernelapp as app\n",
      "/home/ec2-user/anaconda3/envs/tensorflow_p36/lib/python3.6/site-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "ename": "LookupError",
     "evalue": "unknown encoding: unicode",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mLookupError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-40-84ecb9c0238a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mterm\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mtoken\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m             \u001b[0mm\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'utf-8'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'unicode'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mterm\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscore\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mLookupError\u001b[0m: unknown encoding: unicode"
     ]
    }
   ],
   "source": [
    "if token in n_model.wv:\n",
    "    most_similar = n_model.wv.most_similar( token, topn=10 )\n",
    "    for term, score in most_similar:\n",
    "        term = clean_str(term).replace(\" \", \"_\")\n",
    "        if term != token:\n",
    "            m=term.encode('utf-8')\n",
    "            print(m.decode('unicode'))\n",
    "            print(term, score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
