{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer,TfidfVectorizer,TfidfTransformer\n",
    "from sklearn.decomposition import TruncatedSVD,NMF, LatentDirichletAllocation\n",
    "from sklearn.cluster import KMeans\n",
    "import gensim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "w2v_model=gensim.models.KeyedVectors.load_word2vec_format('../../../word2vec/GoogleNews-vectors-negative300.bin',binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_clearcut_topics():\n",
    "    ## for demostration purpose, don't take it personally : )\n",
    "    return np.repeat([\"we love bergers\", \"we hate sandwiches\"], [1000, 1000])\n",
    "\n",
    "def generate_unbalanced_topics():\n",
    "    return np.repeat([\"we love bergers\", \"we hate sandwiches\"], [10, 1000])\n",
    "\n",
    "def generate_semantic_context_topics():\n",
    "    return np.repeat([\"we love bergers\"\n",
    "                      , \"we hate bergers\"\n",
    "                      , \"we love sandwiches\"\n",
    "                      , \"we hate sandwiches\"], 1000)\n",
    "\n",
    "def generate_noisy_topics():\n",
    "    def _random_typos(word, n):\n",
    "        typo_index = np.random.randint(0, len(word), n)\n",
    "        return [word[:i]+\"X\"+word[i+1:] for i in typo_index]\n",
    "    t1 = [\"we love %s\" % w for w in _random_typos(\"bergers\", 15)]\n",
    "    t2 = [\"we hate %s\" % w for w in _random_typos(\"sandwiches\", 15)]\n",
    "    return np.r_[t1, t2]\n",
    "\n",
    "sample_texts = {\n",
    "     \"clearcut topics\": generate_clearcut_topics()\n",
    "    , \"unbalanced topics\": generate_unbalanced_topics()\n",
    "    , \"semantic topics\": generate_semantic_context_topics()\n",
    "    , \"noisy topics\": generate_noisy_topics()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clearcut topics': array(['we love bergers', 'we love bergers', 'we love bergers', ...,\n",
       "        'we hate sandwiches', 'we hate sandwiches', 'we hate sandwiches'],\n",
       "       dtype='<U18'),\n",
       " 'unbalanced topics': array(['we love bergers', 'we love bergers', 'we love bergers', ...,\n",
       "        'we hate sandwiches', 'we hate sandwiches', 'we hate sandwiches'],\n",
       "       dtype='<U18'),\n",
       " 'semantic topics': array(['we love bergers', 'we love bergers', 'we love bergers', ...,\n",
       "        'we hate sandwiches', 'we hate sandwiches', 'we hate sandwiches'],\n",
       "       dtype='<U18'),\n",
       " 'noisy topics': array(['we love bergXrs', 'we love berXers', 'we love bergerX',\n",
       "        'we love bergeXs', 'we love bergXrs', 'we love Xergers',\n",
       "        'we love berXers', 'we love berXers', 'we love bergerX',\n",
       "        'we love bergerX', 'we love Xergers', 'we love bXrgers',\n",
       "        'we love bergeXs', 'we love Xergers', 'we love bergeXs',\n",
       "        'we hate sanXwiches', 'we hate sandwicheX', 'we hate Xandwiches',\n",
       "        'we hate sandwicXes', 'we hate sandwichXs', 'we hate sanXwiches',\n",
       "        'we hate sandXiches', 'we hate sandwicXes', 'we hate sandwicXes',\n",
       "        'we hate sandwichXs', 'we hate sXndwiches', 'we hate Xandwiches',\n",
       "        'we hate sanXwiches', 'we hate saXdwiches', 'we hate sandwicheX'],\n",
       "       dtype='<U18')}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "clearcut topics\n",
      "[('we love bergers', 1000), ('we hate sandwiches', 1000)]\n",
      "\n",
      "unbalanced topics\n",
      "[('we hate sandwiches', 1000), ('we love bergers', 10)]\n",
      "\n",
      "semantic topics\n",
      "[('we love bergers', 1000), ('we hate bergers', 1000), ('we love sandwiches', 1000), ('we hate sandwiches', 1000)]\n",
      "\n",
      "noisy topics\n",
      "[('we love berXers', 3), ('we love bergerX', 3), ('we love bergeXs', 3), ('we love Xergers', 3), ('we hate sanXwiches', 3), ('we hate sandwicXes', 3), ('we love bergXrs', 2), ('we hate sandwicheX', 2), ('we hate Xandwiches', 2), ('we hate sandwichXs', 2), ('we love bXrgers', 1), ('we hate sandXiches', 1), ('we hate sXndwiches', 1), ('we hate saXdwiches', 1)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "for desc, texts in sample_texts.items():\n",
    "    print(desc)\n",
    "    print(Counter(texts).most_common())\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_topic(texts, topic_model, n_topics, vec_model=\"tf\", thr=1e-2, **kwargs):\n",
    "    \"\"\"Return a list of topics from texts by topic models - for demostration of simple data\n",
    "    texts: array-like strings\n",
    "    topic_model: {\"nmf\", \"svd\", \"lda\", \"kmeans\"} for LSA_NMF, LSA_SVD, LDA, KMEANS (not actually a topic model)\n",
    "    n_topics: # of topics in texts\n",
    "    vec_model: {\"tf\", \"tfidf\"} for term_freq, term_freq_inverse_doc_freq\n",
    "    thr: threshold for finding keywords in a topic model\n",
    "    \"\"\"\n",
    "    ## 1. vectorization\n",
    "    vectorizer = CountVectorizer() if vec_model == \"tf\" else TfidfVectorizer()\n",
    "    text_vec = vectorizer.fit_transform(texts)\n",
    "    words = np.array(vectorizer.get_feature_names())\n",
    "    ## 2. topic finding\n",
    "    topic_models = {\"nmf\": NMF, \"svd\": TruncatedSVD, \"lda\": LatentDirichletAllocation, \"kmeans\": KMeans}\n",
    "    topicfinder = topic_models[topic_model](n_topics, **kwargs).fit(text_vec)\n",
    "    topic_dists = topicfinder.components_ if topic_model is not \"kmeans\" else topicfinder.cluster_centers_\n",
    "    topic_dists /= topic_dists.max(axis = 1).reshape((-1, 1))   \n",
    "    ## 3. keywords for topics\n",
    "    ## Unlike other models, LSA_SVD will generate both positive and negative values in topic_word distribution,\n",
    "    ## which makes it more ambiguous to choose keywords for topics. The sign of the weights are kept with the\n",
    "    ## words for a demostration here\n",
    "    def _topic_keywords(topic_dist):\n",
    "        keywords_index = np.abs(topic_dist) >= thr\n",
    "        keywords_prefix = np.where(np.sign(topic_dist) > 0, \"\", \"^\")[keywords_index]\n",
    "        keywords = \" | \".join(map(lambda x: \"\".join(x), zip(keywords_prefix, words[keywords_index])))\n",
    "        return keywords\n",
    "    \n",
    "    topic_keywords = map(_topic_keywords, topic_dists)\n",
    "    return \"\\n\".join(\"Topic %i: %s\" % (i, t) for i, t in enumerate(topic_keywords))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: bergers | hate | love | sandwiches | we\n",
      "Topic 1: ^bergers | hate | ^love | sandwiches\n",
      "Topic 2: ^bergers | ^hate | ^love | ^sandwiches | we\n",
      "Topic 3: bergers | hate | ^love | ^sandwiches\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts[\"clearcut topics\"], \"svd\", 4, vec_model=\"tf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic 0: bergers | hate | love | sandwiches | we\n",
      "Topic 1: bergers | ^hate | love | ^sandwiches\n",
      "Topic 2: bergers | hate | love | sandwiches | ^we\n",
      "Topic 3: bergers | hate | ^love | ^sandwiches\n"
     ]
    }
   ],
   "source": [
    "print(find_topic(sample_texts[\"clearcut topics\"], \"svd\", 4, vec_model=\"tfidf\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tensorflow_p36]",
   "language": "python",
   "name": "conda-env-tensorflow_p36-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
