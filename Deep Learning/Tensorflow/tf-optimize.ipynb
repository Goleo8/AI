{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python/3.7.1/Frameworks/Python.framework/Versions/3.7/lib/python3.7/importlib/_bootstrap.py:219: RuntimeWarning: compiletime version 3.6 of module 'tensorflow.python.framework.fast_tensor_util' does not match runtime version 3.7\n",
      "  return f(*args, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.datasets import fetch_mldata, load_svmlight_file\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "minist_data = input_data.read_data_sets('/tmp/data/')\n",
    "X_train_all=minist_data.train.images\n",
    "y_train_all=minist_data.train.labels.astype('int')\n",
    "X_test_all=minist_data.test.images\n",
    "y_test_all=minist_data.test.labels.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train_1=X_train_all[y_train_all<5]\n",
    "y_train_1=y_train_all[y_train_all<5]\n",
    "X_train_2=X_train_all[y_train_all>=5]\n",
    "y_train_2=y_train_all[y_train_all>=5]\n",
    "X_test_1=X_test_all[y_test_all<5]\n",
    "y_test_1=y_test_all[y_test_all<5]\n",
    "X_test_2=X_test_all[y_test_all>=5]\n",
    "y_test_2=y_test_all[y_test_all>=5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:nan,acc_train:nan,acc_test:0.955828\n",
      "loss:nan,acc_train:nan,acc_test:0.922358\n",
      "loss:nan,acc_train:nan,acc_test:0.960693\n",
      "loss:nan,acc_train:nan,acc_test:0.988519\n",
      "loss:nan,acc_train:nan,acc_test:0.878186\n",
      "loss:nan,acc_train:nan,acc_test:0.983265\n",
      "loss:nan,acc_train:nan,acc_test:0.986573\n",
      "loss:nan,acc_train:nan,acc_test:0.985990\n",
      "loss:nan,acc_train:nan,acc_test:0.975871\n",
      "0.9885192\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "timestamp = datetime.utcnow().strftime('%y-%M-%d')\n",
    "root_dir = 'log_dir'\n",
    "log_dir = \"{}/run-{}\".format(root_dir,timestamp)\n",
    "from functools import partial\n",
    "learning_rate=0.001\n",
    "n_h1=100\n",
    "n_h2=100\n",
    "n_h3=100\n",
    "n_h4=100\n",
    "n_h5=100\n",
    "n_o=5\n",
    "epoch=30\n",
    "batch=64\n",
    "loop=X_train.shape[0]//batch+1\n",
    "best_test_acc=0\n",
    "try_times=5\n",
    "stop=False\n",
    "\n",
    "\n",
    "def shuffle_data(X,y):\n",
    "    index = np.random.permutation(X.shape[0])\n",
    "    return X[index],y[index]\n",
    "\n",
    "def get_batch(X,y,index,batch):\n",
    "    end = (index+1)*batch if (index+1)*batch < X.shape[0] else X.shape[0]-1\n",
    "    return X[index*batch:end],y[index*batch:end]\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "X=tf.placeholder(tf.float32,shape=[None,X_train.shape[1]],name='X')\n",
    "y=tf.placeholder(tf.int32,shape=[None,],name='y')\n",
    "\n",
    "layer=partial(tf.layers.dense,activation=tf.nn.elu,kernel_initializer=tf.variance_scaling_initializer(mode='fan_avg'))\n",
    "with tf.name_scope('dnn'):\n",
    "    h1=layer(X,n_h1,name='h1')\n",
    "    h2=layer(h1,n_h2,name='h2')\n",
    "    h3=layer(h2,n_h3,name='h3')\n",
    "    h4=layer(h3,n_h4,name='h4')\n",
    "    h5=layer(h4,n_h5,name='h5')    \n",
    "    logits=tf.layers.dense(h5,n_o,name='logits')\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    xentropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits))\n",
    "    loss=tf.reduce_mean(xentropy)\n",
    "    \n",
    "    training=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    b=tf.nn.in_top_k(logits,y,1,name='in-top-k')\n",
    "    accuracy=tf.reduce_mean(tf.cast(b,tf.float32),name='eva')\n",
    "\n",
    "with tf.name_scope('log'):\n",
    "    saver = tf.train.Saver()\n",
    "    acc_summary=tf.summary.scalar('acc',accuracy)\n",
    "    file_writer = tf.summary.FileWriter(log_dir,tf.get_default_graph())\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    n_try=0\n",
    "    for e in range(epoch):\n",
    "        X_tmp,y_tmp=shuffle_data(X_train_1,y_train_1)\n",
    "        for l in range(loop):\n",
    "            X_batch,y_batch=get_batch(X_tmp,y_tmp,l,batch)\n",
    "            sess.run(training,feed_dict={X:X_batch,y:y_batch})\n",
    "        test_acc=accuracy.eval({X:X_test_1,y:y_test_1})\n",
    "        print('loss:%f,acc_train:%f,acc_test:%f'%(loss.eval({X:X_batch,y:y_batch}),accuracy.eval({X:X_batch,y:y_batch}),test_acc))\n",
    "        if test_acc>best_test_acc:\n",
    "            best_test_acc=test_acc\n",
    "            n_try=0\n",
    "            saver.save(sess,'models/lm.ckpt')\n",
    "            acc_eval=acc_summary.eval({X:X_test_1,y:y_test_1})\n",
    "            file_writer.add_summary(acc_eval,l)\n",
    "        elif n_try+1<try_times:\n",
    "            n_try+=1\n",
    "        else:\n",
    "            stop=True\n",
    "            print(best_test_acc)\n",
    "            break                   \n",
    "        saver.save(sess,'models/lm_final.ckpt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data=fetch_california_housing()\n",
    "X,y=data.data,data.target\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.3,random_state=42)\n",
    "sc=StandardScaler()\n",
    "X_train_s=sc.fit_transform(X_train)\n",
    "X_test_s=sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3310632\n",
      "1.3253576\n",
      "1.3074894\n",
      "1.3116977\n",
      "1.3038616\n",
      "1.3139673\n",
      "1.3089169\n",
      "1.3307462\n",
      "1.3106983\n",
      "1.3171688\n",
      "1.3058139\n",
      "1.4366393\n",
      "1.3398427\n",
      "1.3126721\n",
      "1.3133603\n",
      "1.3120388\n",
      "1.3215576\n",
      "1.3064449\n",
      "1.3224119\n",
      "1.3476644\n"
     ]
    }
   ],
   "source": [
    "learning_rate=0.1\n",
    "momentum=0.9\n",
    "n_features=X_train.shape[1]\n",
    "n_data=X_train.shape[0]\n",
    "n_h1=10\n",
    "n_h2=10\n",
    "n_h3=1\n",
    "epoch=20\n",
    "batch=64\n",
    "\n",
    "tf.reset_default_graph()\n",
    "X=tf.placeholder(tf.float32,shape=[None,n_features],name='X')\n",
    "y=tf.placeholder(tf.float32,shape=[None,],name='y')\n",
    "\n",
    "initializer = tf.variance_scaling_initializer(mode='fan_avg')\n",
    "h1=tf.layers.dense(X,n_h1,kernel_initializer=initializer,activation=tf.nn.elu,name='h1')\n",
    "h2=tf.layers.dense(h1,n_h2,kernel_initializer=initializer,activation=tf.nn.elu,name='h2')\n",
    "h3=tf.layers.dense(h2,n_h3,kernel_initializer=initializer,name='h3')\n",
    "\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    err=y-h3\n",
    "    loss=tf.reduce_mean(tf.square(err),name='mse')\n",
    "    opt=tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=momentum).minimize(loss)\n",
    "\n",
    "# with tf.name_scope('eval'):\n",
    "\n",
    "def shuffle_data(X,y):\n",
    "    index = np.random.permutation(n_data)\n",
    "    return X[index],y[index]\n",
    "\n",
    "def get_batch(X,y,index,batch):\n",
    "    return X[index*batch:(index+1)*batch],y[index*batch:(index+1)*batch]\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for e in range(epoch):\n",
    "        n_round=n_data//batch\n",
    "        X_tmp,y_tmp=shuffle_data(X_train_s,y_train)\n",
    "        for n in range(n_round):\n",
    "            X_batch,y_batch=get_batch(X_tmp,y_tmp,n,batch)\n",
    "            sess.run(opt,feed_dict={X:X_batch,y:y_batch})\n",
    "        print(loss.eval(feed_dict={X:X_test_s,y:y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
