{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler,OneHotEncoder\n",
    "from sklearn.datasets import fetch_mldata, load_svmlight_file\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-17-01957aae67ad>:4: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /Users/Goleo8/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /Users/Goleo8/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /Users/Goleo8/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /Users/Goleo8/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /Users/Goleo8/anaconda3/lib/python3.6/site-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n"
     ]
    }
   ],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "minist_data = input_data.read_data_sets('/tmp/data/')\n",
    "X_train=minist_data.train.images\n",
    "y_train=minist_data.train.labels.astype('int')\n",
    "X_test=minist_data.test.images\n",
    "y_test=minist_data.test.labels.astype('int')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loss:0.450090,acc_train:0.913043,acc_test:0.946100\n",
      "loss:0.024185,acc_train:1.000000,acc_test:0.960400\n",
      "loss:0.130232,acc_train:0.956522,acc_test:0.965200\n",
      "loss:0.004517,acc_train:1.000000,acc_test:0.969600\n",
      "loss:0.033703,acc_train:1.000000,acc_test:0.974000\n",
      "loss:0.005785,acc_train:1.000000,acc_test:0.972000\n",
      "loss:0.013416,acc_train:1.000000,acc_test:0.974100\n",
      "loss:0.067975,acc_train:1.000000,acc_test:0.976900\n",
      "loss:0.003312,acc_train:1.000000,acc_test:0.971500\n",
      "loss:0.010555,acc_train:1.000000,acc_test:0.975400\n",
      "loss:0.003648,acc_train:1.000000,acc_test:0.975000\n",
      "loss:0.000097,acc_train:1.000000,acc_test:0.978500\n",
      "loss:0.006828,acc_train:1.000000,acc_test:0.977000\n",
      "loss:0.001430,acc_train:1.000000,acc_test:0.974200\n",
      "loss:0.032663,acc_train:1.000000,acc_test:0.975800\n",
      "loss:0.004330,acc_train:1.000000,acc_test:0.976700\n",
      "loss:0.005661,acc_train:1.000000,acc_test:0.976000\n",
      "loss:0.000060,acc_train:1.000000,acc_test:0.973300\n",
      "0.9785\n"
     ]
    }
   ],
   "source": [
    "from functools import partial\n",
    "learning_rate=0.001\n",
    "n_h1=100\n",
    "n_h2=100\n",
    "n_h3=100\n",
    "n_h4=100\n",
    "n_h5=100\n",
    "n_o=10\n",
    "epoch=30\n",
    "batch=64\n",
    "loop=X_train.shape[0]//batch+1\n",
    "best_test_acc=0\n",
    "try_times=5\n",
    "stop=False\n",
    "\n",
    "def shuffle_data(X,y):\n",
    "    index = np.random.permutation(X.shape[0])\n",
    "    return X[index],y[index]\n",
    "\n",
    "def get_batch(X,y,index,batch):\n",
    "    end = (index+1)*batch if (index+1)*batch < X.shape[0] else X.shape[0]-1\n",
    "    return X[index*batch:end],y[index*batch:end]\n",
    "\n",
    "\n",
    "tf.reset_default_graph()\n",
    "X=tf.placeholder(tf.float32,shape=[None,X_train.shape[1]],name='X')\n",
    "y=tf.placeholder(tf.int32,shape=[None,],name='y')\n",
    "\n",
    "layer=partial(tf.layers.dense,activation=tf.nn.elu,kernel_initializer=tf.variance_scaling_initializer(mode='fan_avg'))\n",
    "with tf.name_scope('dnn'):\n",
    "    h1=layer(X,n_h1,name='h1')\n",
    "    h2=layer(h1,n_h2,name='h2')\n",
    "    h3=layer(h2,n_h3,name='h3')\n",
    "    h4=layer(h3,n_h4,name='h4')\n",
    "    h5=layer(h4,n_h5,name='h5')    \n",
    "    logits=tf.layers.dense(h5,n_o,name='logits')\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    xentropy = tf.reduce_mean(tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y,logits=logits))\n",
    "    loss=tf.reduce_mean(xentropy)\n",
    "    \n",
    "    training=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)\n",
    "\n",
    "with tf.name_scope('eval'):\n",
    "    b=tf.nn.in_top_k(logits,y,1,name='in-top-k')\n",
    "    accuracy=tf.reduce_mean(tf.cast(b,tf.float32),name='eva')\n",
    "\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    n_try=0\n",
    "    for e in range(epoch):\n",
    "        X_tmp,y_tmp=shuffle_data(X_train,y_train)\n",
    "        for l in range(loop):\n",
    "            X_batch,y_batch=get_batch(X_tmp,y_tmp,l,batch)\n",
    "            sess.run(training,feed_dict={X:X_batch,y:y_batch})\n",
    "        test_acc=accuracy.eval({X:minist_data.test.images,y:minist_data.test.labels})\n",
    "        print('loss:%f,acc_train:%f,acc_test:%f'%(loss.eval({X:X_batch,y:y_batch}),accuracy.eval({X:X_batch,y:y_batch}),test_acc))\n",
    "        if test_acc>best_test_acc:\n",
    "            best_test_acc=test_acc\n",
    "            n_try=0\n",
    "        elif n_try<try_times:\n",
    "            n_try+=1\n",
    "        else:\n",
    "            stop=True\n",
    "            print(best_test_acc)\n",
    "            break                   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=fetch_california_housing()\n",
    "X,y=data.data,data.target\n",
    "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=.3,random_state=42)\n",
    "sc=StandardScaler()\n",
    "X_train_s=sc.fit_transform(X_train)\n",
    "X_test_s=sc.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3310632\n",
      "1.3253576\n",
      "1.3074894\n",
      "1.3116977\n",
      "1.3038616\n",
      "1.3139673\n",
      "1.3089169\n",
      "1.3307462\n",
      "1.3106983\n",
      "1.3171688\n",
      "1.3058139\n",
      "1.4366393\n",
      "1.3398427\n",
      "1.3126721\n",
      "1.3133603\n",
      "1.3120388\n",
      "1.3215576\n",
      "1.3064449\n",
      "1.3224119\n",
      "1.3476644\n"
     ]
    }
   ],
   "source": [
    "learning_rate=0.1\n",
    "momentum=0.9\n",
    "n_features=X_train.shape[1]\n",
    "n_data=X_train.shape[0]\n",
    "n_h1=10\n",
    "n_h2=10\n",
    "n_h3=1\n",
    "epoch=20\n",
    "batch=64\n",
    "\n",
    "tf.reset_default_graph()\n",
    "X=tf.placeholder(tf.float32,shape=[None,n_features],name='X')\n",
    "y=tf.placeholder(tf.float32,shape=[None,],name='y')\n",
    "\n",
    "initializer = tf.variance_scaling_initializer(mode='fan_avg')\n",
    "h1=tf.layers.dense(X,n_h1,kernel_initializer=initializer,activation=tf.nn.elu,name='h1')\n",
    "h2=tf.layers.dense(h1,n_h2,kernel_initializer=initializer,activation=tf.nn.elu,name='h2')\n",
    "h3=tf.layers.dense(h2,n_h3,kernel_initializer=initializer,name='h3')\n",
    "\n",
    "\n",
    "with tf.name_scope('train'):\n",
    "    err=y-h3\n",
    "    loss=tf.reduce_mean(tf.square(err),name='mse')\n",
    "    opt=tf.train.MomentumOptimizer(learning_rate=learning_rate,momentum=momentum).minimize(loss)\n",
    "\n",
    "# with tf.name_scope('eval'):\n",
    "\n",
    "def shuffle_data(X,y):\n",
    "    index = np.random.permutation(n_data)\n",
    "    return X[index],y[index]\n",
    "\n",
    "def get_batch(X,y,index,batch):\n",
    "    return X[index*batch:(index+1)*batch],y[index*batch:(index+1)*batch]\n",
    "\n",
    "init=tf.global_variables_initializer()\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    for e in range(epoch):\n",
    "        n_round=n_data//batch\n",
    "        X_tmp,y_tmp=shuffle_data(X_train_s,y_train)\n",
    "        for n in range(n_round):\n",
    "            X_batch,y_batch=get_batch(X_tmp,y_tmp,n,batch)\n",
    "            sess.run(opt,feed_dict={X:X_batch,y:y_batch})\n",
    "        print(loss.eval(feed_dict={X:X_test_s,y:y_test}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
