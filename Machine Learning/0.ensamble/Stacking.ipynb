{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, precision_score,confusion_matrix,classification_report\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.datasets import load_svmlight_file\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import resample\n",
    "from sklearn.ensemble import RandomTreesEmbedding\n",
    "from sklearn.preprocessing import PolynomialFeatures\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_svmlight_file('data/demo-g1.txt')\n",
    "\n",
    "X,y=data[0],data[1]\n",
    "\n",
    "X_s,y_s=resample(X,y,n_samples=50000,replace=False)\n",
    "X_train,X_test,y_train,y_test=train_test_split(X_s,y_s,test_size=.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=1000, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr = LogisticRegression(C=10,max_iter=1000)\n",
    "lr.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.66      0.85      0.74      8587\n",
      "         2.0       0.68      0.41      0.51      6413\n",
      "\n",
      "   micro avg       0.66      0.66      0.66     15000\n",
      "   macro avg       0.67      0.63      0.63     15000\n",
      "weighted avg       0.67      0.66      0.64     15000\n",
      "\n",
      "[[7318 3777]\n",
      " [1269 2636]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_test=lr.predict(X_test)\n",
    "print(classification_report(y_test,y_pred_test))\n",
    "print(confusion_matrix(y_test,y_pred_test).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_en=RandomTreesEmbedding(300,n_jobs=-1)\n",
    "X_train_em=rf_en.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "poly = PolynomialFeatures()\n",
    "X_train_poly=poly.fit_transform(X_train_em)\n",
    "X_test_poly = poly.transform(X_test)\n",
    "lr.fit(X_train_poly,y_train)\n",
    "y_pred_test_poly=lr.predict(X_test_poly)\n",
    "print(classification_report(y_test,y_pred_test_poly))\n",
    "print(confusion_matrix(y_test,y_pred_test_poly).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=10, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=1000, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_em = LogisticRegression(C=10,max_iter=1000)\n",
    "lr_em.fit(X_train_em,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.68      0.88      0.77     19705\n",
      "         2.0       0.75      0.46      0.57     15295\n",
      "\n",
      "   micro avg       0.70      0.70      0.70     35000\n",
      "   macro avg       0.71      0.67      0.67     35000\n",
      "weighted avg       0.71      0.70      0.68     35000\n",
      "\n",
      "[[17373  8277]\n",
      " [ 2332  7018]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_train=lr.predict(X_train)\n",
    "print(classification_report(y_train,y_pred_train))\n",
    "print(confusion_matrix(y_train,y_pred_train).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.60      0.97      0.75     19705\n",
      "         2.0       0.84      0.18      0.30     15295\n",
      "\n",
      "   micro avg       0.63      0.63      0.63     35000\n",
      "   macro avg       0.72      0.58      0.52     35000\n",
      "weighted avg       0.71      0.63      0.55     35000\n",
      "\n",
      "[[19184 12543]\n",
      " [  521  2752]]\n"
     ]
    }
   ],
   "source": [
    "y_pred_train_em=lr_em.predict(X_train_em)\n",
    "print(classification_report(y_train,y_pred_train_em))\n",
    "print(confusion_matrix(y_train,y_pred_train_em).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.59      0.95      0.73      8587\n",
      "         2.0       0.64      0.12      0.20      6413\n",
      "\n",
      "   micro avg       0.59      0.59      0.59     15000\n",
      "   macro avg       0.62      0.53      0.46     15000\n",
      "weighted avg       0.61      0.59      0.50     15000\n",
      "\n",
      "[[8157 5649]\n",
      " [ 430  764]]\n"
     ]
    }
   ],
   "source": [
    "X_test_em=rf_en.transform(X_test)\n",
    "y_pred_test_em=lr_em.predict(X_test_em)\n",
    "print(classification_report(y_test,y_pred_test_em))\n",
    "print(confusion_matrix(y_test,y_pred_test_em).T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15000, 18338), (15000, 9531))"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, X_test_em.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test_imp_en=rf_en_new.transform(X_test_imp)\n",
    "y_pred_test_imp_en=lr_en.predict(X_test_imp_en)\n",
    "print(classification_report(y_test,y_pred_test_imp_en))\n",
    "print(confusion_matrix(y_test,y_pred_test_imp_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "length = len(rf_en.feature_importances_)\n",
    "index = range(length)\n",
    "important_features=sorted(zip(index,rf_en.feature_importances_),key=lambda x:x[1],reverse=True)[:500]\n",
    "feature_index = list(map(lambda x:x[0],important_features))\n",
    "X_imp=X[:,feature_index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_imp,X_test_imp,y_train,y_test=train_test_split(X_imp,y,test_size=.3,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_en_new=RandomTreesEmbedding(100,n_jobs=-1)\n",
    "X_train_imp_en=rf_en_new.fit_transform(X_train_imp,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lr_en = LogisticRegression()\n",
    "lr_en.fit(X_train_imp_en,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         1.0       0.57      0.97      0.72    118527\n",
      "         2.0       0.56      0.04      0.08     91479\n",
      "\n",
      "   micro avg       0.57      0.57      0.57    210006\n",
      "   macro avg       0.56      0.51      0.40    210006\n",
      "weighted avg       0.56      0.57      0.44    210006\n",
      "\n",
      "[[115341   3186]\n",
      " [ 87470   4009]]\n"
     ]
    }
   ],
   "source": [
    "X_test_imp_en=rf_en_new.transform(X_test_imp)\n",
    "y_pred_test_imp_en=lr_en.predict(X_test_imp_en)\n",
    "print(classification_report(y_test,y_pred_test_imp_en))\n",
    "print(confusion_matrix(y_test,y_pred_test_imp_en))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((15000, 9458), (15000, 18338))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test_en.shape,X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Goleo8/anaconda3/lib/python3.6/site-packages/lightgbm/engine.py:116: UserWarning: Found `num_trees` in params. Will use it instead of argument\n",
      "  warnings.warn(\"Found `{}` in params. Will use it instead of argument\".format(alias))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's binary_logloss: 34.5388\n",
      "[2]\ttraining's binary_logloss: 34.5388\n",
      "[3]\ttraining's binary_logloss: 34.5388\n",
      "[4]\ttraining's binary_logloss: 34.5388\n",
      "[5]\ttraining's binary_logloss: 34.5388\n",
      "[6]\ttraining's binary_logloss: 34.5388\n",
      "[7]\ttraining's binary_logloss: 34.5388\n",
      "[8]\ttraining's binary_logloss: 34.5388\n",
      "[9]\ttraining's binary_logloss: 34.5388\n",
      "[10]\ttraining's binary_logloss: 34.5388\n",
      "[11]\ttraining's binary_logloss: 34.5388\n",
      "[12]\ttraining's binary_logloss: 34.5388\n",
      "[13]\ttraining's binary_logloss: 34.5388\n",
      "[14]\ttraining's binary_logloss: 34.5388\n",
      "[15]\ttraining's binary_logloss: 34.5388\n",
      "[16]\ttraining's binary_logloss: 34.5388\n",
      "[17]\ttraining's binary_logloss: 34.5388\n",
      "[18]\ttraining's binary_logloss: 34.5388\n",
      "[19]\ttraining's binary_logloss: 34.5388\n",
      "[20]\ttraining's binary_logloss: 34.5388\n",
      "[21]\ttraining's binary_logloss: 34.5388\n",
      "[22]\ttraining's binary_logloss: 34.5388\n",
      "[23]\ttraining's binary_logloss: 34.5388\n",
      "[24]\ttraining's binary_logloss: 34.5388\n",
      "[25]\ttraining's binary_logloss: 34.5388\n",
      "[26]\ttraining's binary_logloss: 34.5388\n",
      "[27]\ttraining's binary_logloss: 34.5388\n",
      "[28]\ttraining's binary_logloss: 34.5388\n",
      "[29]\ttraining's binary_logloss: 34.5388\n",
      "[30]\ttraining's binary_logloss: 34.5388\n",
      "[31]\ttraining's binary_logloss: 34.5388\n",
      "[32]\ttraining's binary_logloss: 34.5388\n",
      "[33]\ttraining's binary_logloss: 34.5388\n",
      "[34]\ttraining's binary_logloss: 34.5388\n",
      "[35]\ttraining's binary_logloss: 34.5388\n",
      "[36]\ttraining's binary_logloss: 34.5388\n",
      "[37]\ttraining's binary_logloss: 34.5388\n",
      "[38]\ttraining's binary_logloss: 34.5388\n",
      "[39]\ttraining's binary_logloss: 34.5388\n",
      "[40]\ttraining's binary_logloss: 34.5388\n",
      "[41]\ttraining's binary_logloss: 34.5388\n",
      "[42]\ttraining's binary_logloss: 34.5388\n",
      "[43]\ttraining's binary_logloss: 34.5388\n",
      "[44]\ttraining's binary_logloss: 34.5388\n",
      "[45]\ttraining's binary_logloss: 34.5388\n",
      "[46]\ttraining's binary_logloss: 34.5388\n",
      "[47]\ttraining's binary_logloss: 34.5388\n",
      "[48]\ttraining's binary_logloss: 34.5388\n",
      "[49]\ttraining's binary_logloss: 34.5388\n",
      "[50]\ttraining's binary_logloss: 34.5388\n",
      "[51]\ttraining's binary_logloss: 34.5388\n",
      "[52]\ttraining's binary_logloss: 34.5388\n",
      "[53]\ttraining's binary_logloss: 34.5388\n",
      "[54]\ttraining's binary_logloss: 34.5388\n",
      "[55]\ttraining's binary_logloss: 34.5388\n",
      "[56]\ttraining's binary_logloss: 34.5388\n",
      "[57]\ttraining's binary_logloss: 34.5388\n",
      "[58]\ttraining's binary_logloss: 34.5388\n",
      "[59]\ttraining's binary_logloss: 34.5388\n",
      "[60]\ttraining's binary_logloss: 34.5388\n",
      "[61]\ttraining's binary_logloss: 34.5388\n",
      "[62]\ttraining's binary_logloss: 34.5388\n",
      "[63]\ttraining's binary_logloss: 34.5388\n",
      "[64]\ttraining's binary_logloss: 34.5388\n",
      "[65]\ttraining's binary_logloss: 34.5388\n",
      "[66]\ttraining's binary_logloss: 34.5388\n",
      "[67]\ttraining's binary_logloss: 34.5388\n",
      "[68]\ttraining's binary_logloss: 34.5388\n",
      "[69]\ttraining's binary_logloss: 34.5388\n",
      "[70]\ttraining's binary_logloss: 34.5388\n",
      "[71]\ttraining's binary_logloss: 34.5388\n",
      "[72]\ttraining's binary_logloss: 34.5388\n",
      "[73]\ttraining's binary_logloss: 34.5388\n",
      "[74]\ttraining's binary_logloss: 34.5388\n",
      "[75]\ttraining's binary_logloss: 34.5388\n",
      "[76]\ttraining's binary_logloss: 34.5388\n",
      "[77]\ttraining's binary_logloss: 34.5388\n",
      "[78]\ttraining's binary_logloss: 34.5388\n",
      "[79]\ttraining's binary_logloss: 34.5388\n",
      "[80]\ttraining's binary_logloss: 34.5388\n",
      "[81]\ttraining's binary_logloss: 34.5388\n",
      "[82]\ttraining's binary_logloss: 34.5388\n",
      "[83]\ttraining's binary_logloss: 34.5388\n",
      "[84]\ttraining's binary_logloss: 34.5388\n",
      "[85]\ttraining's binary_logloss: 34.5388\n",
      "[86]\ttraining's binary_logloss: 34.5388\n",
      "[87]\ttraining's binary_logloss: 34.5388\n",
      "[88]\ttraining's binary_logloss: 34.5388\n",
      "[89]\ttraining's binary_logloss: 34.5388\n",
      "[90]\ttraining's binary_logloss: 34.5388\n",
      "[91]\ttraining's binary_logloss: 34.5388\n",
      "[92]\ttraining's binary_logloss: 34.5388\n",
      "[93]\ttraining's binary_logloss: 34.5388\n",
      "[94]\ttraining's binary_logloss: 34.5388\n",
      "[95]\ttraining's binary_logloss: 34.5388\n",
      "[96]\ttraining's binary_logloss: 34.5388\n",
      "[97]\ttraining's binary_logloss: 34.5388\n",
      "[98]\ttraining's binary_logloss: 34.5388\n",
      "[99]\ttraining's binary_logloss: 34.5388\n",
      "[100]\ttraining's binary_logloss: 34.5388\n",
      "Save model...\n",
      "Start predicting...\n",
      "y_pred [0 0 0 ... 0 0 0]\n",
      "(35000,)\n",
      "[0 0 0 0 0 0 0 0 0 0]\n",
      "Writing transformed training data\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "object of type 'numpy.int32' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-e523afed2fac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m                                        dtype=np.int64)  # N * num_tress * num_leafs\n\u001b[1;32m     65\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m     \u001b[0mtemp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnum_leaf\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m     \u001b[0mtransformed_training_matrix\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtemp\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: object of type 'numpy.int32' has no len()"
     ]
    }
   ],
   "source": [
    "# print('Load data...')\n",
    "# df_train = pd.read_csv('data/train.csv')\n",
    "# df_test = pd.read_csv('data/test.csv')\n",
    "\n",
    "# NUMERIC_COLS = [\n",
    "#     \"ps_reg_01\", \"ps_reg_02\", \"ps_reg_03\",\n",
    "#     \"ps_car_12\", \"ps_car_13\", \"ps_car_14\", \"ps_car_15\",\n",
    "# ]\n",
    "\n",
    "# print(df_test.head(10))\n",
    "\n",
    "\n",
    "\n",
    "# create dataset for lightgbm\n",
    "lgb_train = lgb.Dataset(X_train, y_train)\n",
    "lgb_eval = lgb.Dataset(X_test, y_test, reference=lgb_train)\n",
    "\n",
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': {'binary_logloss'},\n",
    "    'num_leaves': 64,\n",
    "    'num_trees': 100,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# number of leaves,will be used in feature transformation\n",
    "num_leaf = 64\n",
    "\n",
    "print('Start training...')\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=100,\n",
    "                valid_sets=lgb_train)\n",
    "\n",
    "print('Save model...')\n",
    "# save model to file\n",
    "gbm.save_model('model.txt')\n",
    "\n",
    "print('Start predicting...')\n",
    "# predict and get data on leaves, training data\n",
    "y_pred = gbm.predict(X_train, pred_leaf=True)\n",
    "print(\"y_pred\",y_pred)\n",
    "\n",
    "print(np.array(y_pred).shape)\n",
    "print(y_pred[:10])\n",
    "\n",
    "print('Writing transformed training data')\n",
    "# transformed_training_matrix = np.zeros([len(y_pred), len(y_pred[0]) * num_leaf],\n",
    "#                                        dtype=np.int64)  # N * num_tress * num_leafs\n",
    "\n",
    "transformed_training_matrix = np.zeros([len(y_pred), num_leaf],\n",
    "                                       dtype=np.int64)  # N * num_tress * num_leafs\n",
    "for i in range(0, len(y_pred)):\n",
    "    temp = np.arange(len(y_pred[0])) * num_leaf + np.array(y_pred[i])\n",
    "    transformed_training_matrix[i][temp] += 1\n",
    "\n",
    "\n",
    "y_pred = gbm.predict(X_test, pred_leaf=True)\n",
    "print('Writing transformed testing data')\n",
    "transformed_testing_matrix = np.zeros([len(y_pred), len(y_pred[0]) * num_leaf], dtype=np.int64)\n",
    "for i in range(0, len(y_pred)):\n",
    "    temp = np.arange(len(y_pred[0])) * num_leaf + np.array(y_pred[i])\n",
    "    transformed_testing_matrix[i][temp] += 1\n",
    "\n",
    "\n",
    "lm = LogisticRegression(penalty='l2',C=0.05) # logestic model construction\n",
    "lm.fit(transformed_training_matrix,y_train)  # fitting the data\n",
    "y_pred_test = lm.predict_proba(transformed_testing_matrix)   # Give the probabilty on each label\n",
    "\n",
    "print(y_pred_test)\n",
    "\n",
    "NE = (-1) / len(y_pred_test) * sum(((1+y_test)/2 * np.log(y_pred_test[:,1]) +  (1-y_test)/2 * np.log(1 - y_pred_test[:,1])))\n",
    "print(\"Normalized Cross Entropy \" + str(NE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(314581,)"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
