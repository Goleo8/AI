{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../datasets/flight/flight_delays_train.csv')\n",
    "test_df = pd.read_csv('../datasets/flight/flight_delays_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train=train_df[['Distance','DepTime']]\n",
    "y_train = train_df['dep_delayed_15min']\n",
    "X_train_part, X_valid, y_train_part, y_valid = \\\n",
    "    train_test_split(X_train, y_train, \n",
    "                     test_size=0.3, random_state=17)\n",
    "X_test = test_df[['Distance','DepTime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                       ('logit', LogisticRegression(C=1, random_state=17, solver='liblinear'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6795691465352607"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_pipe.fit(X_train_part, y_train_part)\n",
    "logit_valid_pred = logit_pipe.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "roc_auc_score(y_valid, logit_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat_cols = [ i for i in train_df.columns if i not in ['Distance','DepTime','dep_delayed_15min']]\n",
    "\n",
    "df_cat = pd.concat([train_df[cat_cols],test_df[cat_cols]],axis = 0)\n",
    "df_num = pd.concat([train_df[['Distance','DepTime']],test_df[['Distance','DepTime']]],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot=OneHotEncoder()\n",
    "onehot.fit(df_cat)\n",
    "X_train_onehot = onehot.transform(train_df[cat_cols])\n",
    "X_test_onehot = onehot.transform(test_df[cat_cols])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_num)\n",
    "X_train_num = scaler.transform(X_train)\n",
    "X_test_num = scaler.transform(X_test)\n",
    "X_train, y_train = np.hstack([X_train_onehot.toarray(),X_train_num]), train_df['dep_delayed_15min'].map({'Y': 1, 'N': 0}).values\n",
    "\n",
    "\n",
    "X_test = np.hstack([X_train_onehot.toarray(),X_test_num])\n",
    "\n",
    "X_train_part, X_valid, y_train_part, y_valid = \\\n",
    "    train_test_split(X_train, y_train, \n",
    "                     test_size=0.3, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(X_train_part,y_train_part)\n",
    "lgb_eval = lgb.Dataset(X_valid,y_valid,reference=lgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[1]\ttraining's binary_logloss: 0.486785\n",
      "[2]\ttraining's binary_logloss: 0.485722\n",
      "[3]\ttraining's binary_logloss: 0.484719\n",
      "[4]\ttraining's binary_logloss: 0.48375\n",
      "[5]\ttraining's binary_logloss: 0.482772\n",
      "[6]\ttraining's binary_logloss: 0.481828\n",
      "[7]\ttraining's binary_logloss: 0.480922\n",
      "[8]\ttraining's binary_logloss: 0.480026\n",
      "[9]\ttraining's binary_logloss: 0.479151\n",
      "[10]\ttraining's binary_logloss: 0.47831\n",
      "[11]\ttraining's binary_logloss: 0.478085\n",
      "[12]\ttraining's binary_logloss: 0.477243\n",
      "[13]\ttraining's binary_logloss: 0.476432\n",
      "[14]\ttraining's binary_logloss: 0.475639\n",
      "[15]\ttraining's binary_logloss: 0.475412\n",
      "[16]\ttraining's binary_logloss: 0.474615\n",
      "[17]\ttraining's binary_logloss: 0.47383\n",
      "[18]\ttraining's binary_logloss: 0.473066\n",
      "[19]\ttraining's binary_logloss: 0.472305\n",
      "[20]\ttraining's binary_logloss: 0.471567\n",
      "[21]\ttraining's binary_logloss: 0.470863\n",
      "[22]\ttraining's binary_logloss: 0.470171\n",
      "[23]\ttraining's binary_logloss: 0.469492\n",
      "[24]\ttraining's binary_logloss: 0.468833\n",
      "[25]\ttraining's binary_logloss: 0.468182\n",
      "[26]\ttraining's binary_logloss: 0.467978\n",
      "[27]\ttraining's binary_logloss: 0.467329\n",
      "[28]\ttraining's binary_logloss: 0.466694\n",
      "[29]\ttraining's binary_logloss: 0.46608\n",
      "[30]\ttraining's binary_logloss: 0.465481\n",
      "[31]\ttraining's binary_logloss: 0.464886\n",
      "[32]\ttraining's binary_logloss: 0.464295\n",
      "[33]\ttraining's binary_logloss: 0.463715\n",
      "[34]\ttraining's binary_logloss: 0.463145\n",
      "[35]\ttraining's binary_logloss: 0.462582\n",
      "[36]\ttraining's binary_logloss: 0.462033\n",
      "[37]\ttraining's binary_logloss: 0.461483\n",
      "[38]\ttraining's binary_logloss: 0.460936\n",
      "[39]\ttraining's binary_logloss: 0.460406\n",
      "[40]\ttraining's binary_logloss: 0.459895\n",
      "[41]\ttraining's binary_logloss: 0.459367\n",
      "[42]\ttraining's binary_logloss: 0.458864\n",
      "[43]\ttraining's binary_logloss: 0.458358\n",
      "[44]\ttraining's binary_logloss: 0.457867\n",
      "[45]\ttraining's binary_logloss: 0.457381\n",
      "[46]\ttraining's binary_logloss: 0.456906\n",
      "[47]\ttraining's binary_logloss: 0.456439\n",
      "[48]\ttraining's binary_logloss: 0.455968\n",
      "[49]\ttraining's binary_logloss: 0.455517\n",
      "[50]\ttraining's binary_logloss: 0.455061\n",
      "[51]\ttraining's binary_logloss: 0.454623\n",
      "[52]\ttraining's binary_logloss: 0.454203\n",
      "[53]\ttraining's binary_logloss: 0.453784\n",
      "[54]\ttraining's binary_logloss: 0.453367\n",
      "[55]\ttraining's binary_logloss: 0.452968\n",
      "[56]\ttraining's binary_logloss: 0.452558\n",
      "[57]\ttraining's binary_logloss: 0.452158\n",
      "[58]\ttraining's binary_logloss: 0.451963\n",
      "[59]\ttraining's binary_logloss: 0.451561\n",
      "[60]\ttraining's binary_logloss: 0.451373\n",
      "[61]\ttraining's binary_logloss: 0.450979\n",
      "[62]\ttraining's binary_logloss: 0.450588\n",
      "[63]\ttraining's binary_logloss: 0.450201\n",
      "[64]\ttraining's binary_logloss: 0.449829\n",
      "[65]\ttraining's binary_logloss: 0.449464\n",
      "[66]\ttraining's binary_logloss: 0.44909\n",
      "[67]\ttraining's binary_logloss: 0.448722\n",
      "[68]\ttraining's binary_logloss: 0.448361\n",
      "[69]\ttraining's binary_logloss: 0.448001\n",
      "[70]\ttraining's binary_logloss: 0.447654\n",
      "[71]\ttraining's binary_logloss: 0.447309\n",
      "[72]\ttraining's binary_logloss: 0.446971\n",
      "[73]\ttraining's binary_logloss: 0.446644\n",
      "[74]\ttraining's binary_logloss: 0.446474\n",
      "[75]\ttraining's binary_logloss: 0.446143\n",
      "[76]\ttraining's binary_logloss: 0.445813\n",
      "[77]\ttraining's binary_logloss: 0.445484\n",
      "[78]\ttraining's binary_logloss: 0.445172\n",
      "[79]\ttraining's binary_logloss: 0.444849\n",
      "[80]\ttraining's binary_logloss: 0.44454\n",
      "[81]\ttraining's binary_logloss: 0.444237\n",
      "[82]\ttraining's binary_logloss: 0.443938\n",
      "[83]\ttraining's binary_logloss: 0.443643\n",
      "[84]\ttraining's binary_logloss: 0.443359\n",
      "[85]\ttraining's binary_logloss: 0.4432\n",
      "[86]\ttraining's binary_logloss: 0.442912\n",
      "[87]\ttraining's binary_logloss: 0.442628\n",
      "[88]\ttraining's binary_logloss: 0.442468\n",
      "[89]\ttraining's binary_logloss: 0.442184\n",
      "[90]\ttraining's binary_logloss: 0.441907\n",
      "[91]\ttraining's binary_logloss: 0.441631\n",
      "[92]\ttraining's binary_logloss: 0.441363\n",
      "[93]\ttraining's binary_logloss: 0.441098\n",
      "[94]\ttraining's binary_logloss: 0.440829\n",
      "[95]\ttraining's binary_logloss: 0.44057\n",
      "[96]\ttraining's binary_logloss: 0.440315\n",
      "[97]\ttraining's binary_logloss: 0.440067\n",
      "[98]\ttraining's binary_logloss: 0.43982\n",
      "[99]\ttraining's binary_logloss: 0.439571\n",
      "[100]\ttraining's binary_logloss: 0.439326\n",
      "Save model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x110b08438>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': {'binary_logloss'},\n",
    "    'num_leaves': 64,\n",
    "    'num_trees': 100,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# number of leaves,will be used in feature transformation\n",
    "num_leaf = 64\n",
    "\n",
    "print('Start training...')\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=100,\n",
    "                valid_sets=lgb_train)\n",
    "\n",
    "print('Save model...')\n",
    "# save model to file\n",
    "gbm.save_model('lgb-model.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y_pred = gbm.predict(X_train, pred_leaf=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([28, 29, 26, 24, 62, 18, 52, 63, 52, 39, 17, 61, 30, 49, 11, 15, 12,\n",
       "       37, 12, 12, 40, 40, 21, 37, 46,  6, 11, 63,  2, 49,  5, 45, 63, 42,\n",
       "       24,  5,  7, 61,  7,  7, 43, 45, 44, 16, 37,  8,  9, 33, 33, 24, 23,\n",
       "       27, 53, 54, 49, 61, 55, 10,  2, 46, 17, 34, 49, 62, 53, 26, 42, 14,\n",
       "        2, 12, 14, 13, 14, 15, 18, 28, 35, 50, 56, 16, 31, 46,  2,  2,  8,\n",
       "       56, 42, 23, 37, 31, 38, 14, 14, 16, 15, 48, 52, 18, 42, 39],\n",
       "      dtype=int32)"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred[0,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting...\n",
      "(100000, 100)\n",
      "[[28 29 26 24 62 18 52 63 52 39 17 61 30 49 11 15 12 37 12 12 40 40 21 37\n",
      "  46  6 11 63  2 49  5 45 63 42 24  5  7 61  7  7 43 45 44 16 37  8  9 33\n",
      "  33 24 23 27 53 54 49 61 55 10  2 46 17 34 49 62 53 26 42 14  2 12 14 13\n",
      "  14 15 18 28 35 50 56 16 31 46  2  2  8 56 42 23 37 31 38 14 14 16 15 48\n",
      "  52 18 42 39]\n",
      " [17 32 21 20 25 16 13 17 16 11 11 25 20 21  0 18 18 30 31 19 27 29 18 53\n",
      "  30 57 13 13  1 12 17 20 21 21 12  1 24  1  1  1 13 19 13 36 19  8  9 31\n",
      "  10 14 43 46 45  1 39 17 17 10 20  9  1  1  1 41  1 30 53 11 11 10  5 63\n",
      "  29 27 48 52  1 42 58 47 22 22  1 22  8 59 11  5 19 16  1 30 51 41 57 31\n",
      "  35 28 13 13]\n",
      " [17 34 21 20 26 58 13 55 62 11 22 28 57  9  2 52 56 26  1 16 10 10 10 31\n",
      "  10 24 13 13 54 12 13 13 12 12 12  7  8  8  9  9 13  9 13 12 23  1  1 13\n",
      "   9  1  1 14 14 14 50  7  6 27  7  0  1  1  1 12 13  1  1 11 11 10 18 19\n",
      "  15  0 22  1  1 27 12 47  8  8  7  8  1  7  7  1 48 13 14 13 13  8 10 40\n",
      "  40 43  1  1]\n",
      " [ 3  3  3  3  3  3  3  3  3  3 17  3  3  3  0  3  3  3 40  3  3  3  3 61\n",
      "   3  6  3  3  3  3  3  3  3  3  3  3  3  3  3  3  2  2  2  3  2  2  2  2\n",
      "   2  2  2  2  2  2  2 43 41 10 42 56 16 15 15 14  2  2  2 36 40 38 53  3\n",
      "  44 41  3 40  2 22 45 42  3  3  3  3  8 29 29 24  1  1 58  3 61 37 50  3\n",
      "  47 56 48  3]\n",
      " [18 22 16  8 22 30 28 30 33 33 24 21 38 18  0 12 13 14 14 10 51 60 28 18\n",
      "  63 11 12 12 13 13 24  5 16 15 18 25 16 16 16 17 28 20 42 10 31 13 14 43\n",
      "  30  9 38 38 53 44 49  2  2 10 31  8 28 18 27 57 20 44 33 61 14 45 27 27\n",
      "  25 39 28  9  8  8  9  9 12 35 11 32  8 57 13 14 60 35 13 29 38 46 15 31\n",
      "  31 49  5  5]\n",
      " [28 29 26 24 63 18 52 63 52 39 17 61 30 49 23 15 12 37 12 12 40 40 21 37\n",
      "  46  6  7  7  6 31  5 21 63 62 28  5  7 61  7  7 13 11 13 29 19  8  9 54\n",
      "  10 24 23 57 53 54 49 34 61 15 56 46 17 34 19 62 19 50 57 14  2 12 14 13\n",
      "  14 27 18 27 57 18 56 16 31 46  2  2 14 56 42 51 17 31 38 14 14 16 30 48\n",
      "  52 18 42 39]\n",
      " [20 21 19 17 21 17 17 18 18 12 17 11 11 12 45 14 14 15 13 13  8  8  8  8\n",
      "   8 35  9  8  7  8  8  8  8  8  8  8  9  9  8  8 10  8 10  9 10  9  8  8\n",
      "   8  8  7  7  7  7  7 12 11  4 13  0  9  9  9  8  8 11 10 21 22 19 11 12\n",
      "  11  8  8  6  6  6  6  5  6  6  6  6 58  9  8 51  4  4  6  6  6  6  5 10\n",
      "   9  9 11 10]\n",
      " [ 4  4  4  4  4  4  4  4  4  4 11  4  4  4  0  4  4  4  4  4  4  4  4  4\n",
      "   4 57  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4  4\n",
      "   4  4  4  4  4  4  4  4  4 10  4  8  4  4  4  4  4  3  3 21 22 19 11 12\n",
      "  11  0  8  3  3  3  3  3  4  4  4  4  8  4  4  5  3  3  4  4  4  4  4  4\n",
      "   4  4  4  4]\n",
      " [20 21 19 17 21 17 17 18 18 12 22 11 11 12  2 14 14 15 13 13  8  8  8  8\n",
      "   8 16  9  8  7  8  8  8  8  8  8  8  9  9  8  8 10  8 10  9 10  9  8  8\n",
      "   8  8  7  7  7  7  7 12 11 32 13 22  9  9  9  8  8 11 10 21 22 19 11 12\n",
      "  11 31  8  6  6  6  6  5  6  6  6  6  1  9  8 50  4  4  6  6  6  6  5 10\n",
      "   9  9 11 10]\n",
      " [43 45 46 42 28 18 52 19 19 43 17 13 56 58  6 26 29  2 33 30 16 16 16 17\n",
      "  16 54 11 63  2 40 15 15 15 45 17 17 36 53 50 29 27 48 29 16 37 15 16 33\n",
      "  33 24 19 20 22 62 51  2  2 11  2 26 17 34 49 63 53 63 43 63  2 14 23 39\n",
      "  22 16 25 10 26 32 10 10 31 46  2  2 23 13 42 23  2 10 38 14 14 16 15 48\n",
      "  52 18 42 39]]\n",
      "Writing transformed training data\n"
     ]
    }
   ],
   "source": [
    "print('Start predicting...')\n",
    "# predict and get data on leaves, training data\n",
    "y_pred = gbm.predict(X_train, pred_leaf=True)\n",
    "\n",
    "print(np.array(y_pred).shape)\n",
    "print(y_pred[:10])\n",
    "\n",
    "print('Writing transformed training data')\n",
    "transformed_training_matrix = np.zeros([len(y_pred), len(y_pred[0]) * num_leaf],\n",
    "                                       dtype=np.int64)  # N * num_tress * num_leafs\n",
    "for i in range(0, len(y_pred)):\n",
    "    temp = np.arange(len(y_pred[0])) * num_leaf + np.array(y_pred[i])\n",
    "    b[i][temp] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "print('Start predicting...')\n",
    "# predict and get data on leaves, training data\n",
    "y_pred = gbm.predict(X_train, pred_leaf=True)\n",
    "\n",
    "print(np.array(y_pred).shape)\n",
    "print(y_pred[:10])\n",
    "\n",
    "print('Writing transformed training data')\n",
    "transformed_training_matrix = np.zeros([len(y_pred), len(y_pred[0]) * num_leaf],\n",
    "                                       dtype=np.int64)  # N * num_tress * num_leafs\n",
    "for i in range(0, len(y_pred)):\n",
    "    temp = np.arange(len(y_pred[0])) * num_leaf + np.array(y_pred[i])\n",
    "    transformed_training_matrix[i][temp] += 1\n",
    "\n",
    "\n",
    "y_pred = gbm.predict(X_test, pred_leaf=True)\n",
    "print('Writing transformed testing data')\n",
    "transformed_testing_matrix = np.zeros([len(y_pred), len(y_pred[0]) * num_leaf], dtype=np.int64)\n",
    "for i in range(0, len(y_pred)):\n",
    "    temp = np.arange(len(y_pred[0])) * num_leaf + np.array(y_pred[i])\n",
    "    transformed_testing_matrix[i][temp] += 1\n",
    "\n",
    "\n",
    "lm = LogisticRegression(penalty='l2',C=0.05) # logestic model construction\n",
    "lm.fit(transformed_training_matrix,y_train)  # fitting the data\n",
    "y_pred_test = lm.predict_proba(transformed_testing_matrix)   # Give the probabilty on each label\n",
    "\n",
    "print(y_pred_test)\n",
    "\n",
    "NE = (-1) / len(y_pred_test) * sum(((1+y_test)/2 * np.log(y_pred_test[:,1]) +  (1-y_test)/2 * np.log(1 - y_pred_test[:,1])))\n",
    "print(\"Normalized Cross Entropy \" + str(NE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_clf = xgb.XGBClassifier(\n",
    "                 colsample_bytree=0.2,\n",
    "                 gamma=0.0,\n",
    "                 learning_rate=0.01,\n",
    "                 max_depth=8,\n",
    "                 min_child_weight=1.5,\n",
    "                 n_estimators=1000,                                                                  \n",
    "                 reg_alpha=0.9,\n",
    "                 reg_lambda=0.6,\n",
    "                 subsample=0.2,\n",
    "                 num\n",
    "                 seed=42,\n",
    "                 silent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.2, gamma=0.0, learning_rate=0.01,\n",
       "       max_delta_step=0, max_depth=8, min_child_weight=1.5, missing=None,\n",
       "       n_estimators=1000, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0.9,\n",
       "       reg_lambda=0.6, scale_pos_weight=1, seed=42, silent=1,\n",
       "       subsample=0.2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf.fit(X_train_part,y_train_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7313949358315118"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_valid_pred = xgb_clf.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "roc_auc_score(y_valid, xgb_valid_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
