{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('../datasets/flight/flight_delays_train.csv')\n",
    "test_df = pd.read_csv('../datasets/flight/flight_delays_test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train=train_df[['Distance','DepTime']]\n",
    "y_train = train_df['dep_delayed_15min']\n",
    "X_train_part, X_valid, y_train_part, y_valid = \\\n",
    "    train_test_split(X_train, y_train, \n",
    "                     test_size=0.3, random_state=17)\n",
    "X_test = test_df[['Distance','DepTime']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "logit_pipe = Pipeline([('scaler', StandardScaler()),\n",
    "                       ('logit', LogisticRegression(C=1, random_state=17, solver='liblinear'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6795691465352607"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logit_pipe.fit(X_train_part, y_train_part)\n",
    "logit_valid_pred = logit_pipe.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "roc_auc_score(y_valid, logit_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cat_cols = [ i for i in train_df.columns if i not in ['Distance','DepTime','dep_delayed_15min']]\n",
    "\n",
    "df_cat = pd.concat([train_df[cat_cols],test_df[cat_cols]],axis = 0)\n",
    "df_num = pd.concat([train_df[['Distance','DepTime']],test_df[['Distance','DepTime']]],axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "onehot=OneHotEncoder()\n",
    "onehot.fit(df_cat)\n",
    "X_train_onehot = onehot.transform(train_df[cat_cols])\n",
    "X_test_onehot = onehot.transform(test_df[cat_cols])\n",
    "\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(df_num)\n",
    "X_train_num = scaler.transform(X_train)\n",
    "X_test_num = scaler.transform(X_test)\n",
    "X_train, y_train = np.hstack([X_train_onehot.toarray(),X_train_num]), train_df['dep_delayed_15min'].map({'Y': 1, 'N': 0}).values\n",
    "\n",
    "\n",
    "X_test = np.hstack([X_train_onehot.toarray(),X_test_num])\n",
    "\n",
    "X_train_part, X_valid, y_train_part, y_valid = \\\n",
    "    train_test_split(X_train, y_train, \n",
    "                     test_size=0.3, random_state=17)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lgb_train = lgb.Dataset(X_train_part,y_train_part)\n",
    "lgb_eval = lgb.Dataset(X_valid,y_valid,reference=lgb_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(70000, 689)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_part.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start training...\n",
      "[1]\ttraining's binary_logloss: 0.48689\n",
      "[2]\ttraining's binary_logloss: 0.485939\n",
      "[3]\ttraining's binary_logloss: 0.485032\n",
      "[4]\ttraining's binary_logloss: 0.484151\n",
      "[5]\ttraining's binary_logloss: 0.483289\n",
      "[6]\ttraining's binary_logloss: 0.482449\n",
      "[7]\ttraining's binary_logloss: 0.481636\n",
      "[8]\ttraining's binary_logloss: 0.480828\n",
      "[9]\ttraining's binary_logloss: 0.480031\n",
      "[10]\ttraining's binary_logloss: 0.479269\n",
      "[11]\ttraining's binary_logloss: 0.479111\n",
      "[12]\ttraining's binary_logloss: 0.478356\n",
      "[13]\ttraining's binary_logloss: 0.477626\n",
      "[14]\ttraining's binary_logloss: 0.476908\n",
      "[15]\ttraining's binary_logloss: 0.476746\n",
      "[16]\ttraining's binary_logloss: 0.476027\n",
      "[17]\ttraining's binary_logloss: 0.475325\n",
      "[18]\ttraining's binary_logloss: 0.474646\n",
      "[19]\ttraining's binary_logloss: 0.473968\n",
      "[20]\ttraining's binary_logloss: 0.473312\n",
      "[21]\ttraining's binary_logloss: 0.47268\n",
      "[22]\ttraining's binary_logloss: 0.472058\n",
      "[23]\ttraining's binary_logloss: 0.471455\n",
      "[24]\ttraining's binary_logloss: 0.470849\n",
      "[25]\ttraining's binary_logloss: 0.47027\n",
      "[26]\ttraining's binary_logloss: 0.470116\n",
      "[27]\ttraining's binary_logloss: 0.469536\n",
      "[28]\ttraining's binary_logloss: 0.468969\n",
      "[29]\ttraining's binary_logloss: 0.468424\n",
      "[30]\ttraining's binary_logloss: 0.467882\n",
      "[31]\ttraining's binary_logloss: 0.467352\n",
      "[32]\ttraining's binary_logloss: 0.466831\n",
      "[33]\ttraining's binary_logloss: 0.466305\n",
      "[34]\ttraining's binary_logloss: 0.465801\n",
      "[35]\ttraining's binary_logloss: 0.465283\n",
      "[36]\ttraining's binary_logloss: 0.464799\n",
      "[37]\ttraining's binary_logloss: 0.464318\n",
      "[38]\ttraining's binary_logloss: 0.463831\n",
      "[39]\ttraining's binary_logloss: 0.463362\n",
      "[40]\ttraining's binary_logloss: 0.462907\n",
      "[41]\ttraining's binary_logloss: 0.462433\n",
      "[42]\ttraining's binary_logloss: 0.461978\n",
      "[43]\ttraining's binary_logloss: 0.461522\n",
      "[44]\ttraining's binary_logloss: 0.461087\n",
      "[45]\ttraining's binary_logloss: 0.460648\n",
      "[46]\ttraining's binary_logloss: 0.46023\n",
      "[47]\ttraining's binary_logloss: 0.45982\n",
      "[48]\ttraining's binary_logloss: 0.459415\n",
      "[49]\ttraining's binary_logloss: 0.45901\n",
      "[50]\ttraining's binary_logloss: 0.458609\n",
      "[51]\ttraining's binary_logloss: 0.458216\n",
      "[52]\ttraining's binary_logloss: 0.457832\n",
      "[53]\ttraining's binary_logloss: 0.457453\n",
      "[54]\ttraining's binary_logloss: 0.457079\n",
      "[55]\ttraining's binary_logloss: 0.456718\n",
      "[56]\ttraining's binary_logloss: 0.456353\n",
      "[57]\ttraining's binary_logloss: 0.455994\n",
      "[58]\ttraining's binary_logloss: 0.455851\n",
      "[59]\ttraining's binary_logloss: 0.455497\n",
      "[60]\ttraining's binary_logloss: 0.455359\n",
      "[61]\ttraining's binary_logloss: 0.455008\n",
      "[62]\ttraining's binary_logloss: 0.454661\n",
      "[63]\ttraining's binary_logloss: 0.454327\n",
      "[64]\ttraining's binary_logloss: 0.453996\n",
      "[65]\ttraining's binary_logloss: 0.453676\n",
      "[66]\ttraining's binary_logloss: 0.453349\n",
      "[67]\ttraining's binary_logloss: 0.453031\n",
      "[68]\ttraining's binary_logloss: 0.452721\n",
      "[69]\ttraining's binary_logloss: 0.452413\n",
      "[70]\ttraining's binary_logloss: 0.452113\n",
      "[71]\ttraining's binary_logloss: 0.451815\n",
      "[72]\ttraining's binary_logloss: 0.451523\n",
      "[73]\ttraining's binary_logloss: 0.451238\n",
      "[74]\ttraining's binary_logloss: 0.451107\n",
      "[75]\ttraining's binary_logloss: 0.45082\n",
      "[76]\ttraining's binary_logloss: 0.45053\n",
      "[77]\ttraining's binary_logloss: 0.450239\n",
      "[78]\ttraining's binary_logloss: 0.449971\n",
      "[79]\ttraining's binary_logloss: 0.44969\n",
      "[80]\ttraining's binary_logloss: 0.449422\n",
      "[81]\ttraining's binary_logloss: 0.449156\n",
      "[82]\ttraining's binary_logloss: 0.4489\n",
      "[83]\ttraining's binary_logloss: 0.448641\n",
      "[84]\ttraining's binary_logloss: 0.448393\n",
      "[85]\ttraining's binary_logloss: 0.448276\n",
      "[86]\ttraining's binary_logloss: 0.448023\n",
      "[87]\ttraining's binary_logloss: 0.447781\n",
      "[88]\ttraining's binary_logloss: 0.447657\n",
      "[89]\ttraining's binary_logloss: 0.447414\n",
      "[90]\ttraining's binary_logloss: 0.447177\n",
      "[91]\ttraining's binary_logloss: 0.446942\n",
      "[92]\ttraining's binary_logloss: 0.44671\n",
      "[93]\ttraining's binary_logloss: 0.446479\n",
      "[94]\ttraining's binary_logloss: 0.446255\n",
      "[95]\ttraining's binary_logloss: 0.446036\n",
      "[96]\ttraining's binary_logloss: 0.44582\n",
      "[97]\ttraining's binary_logloss: 0.445611\n",
      "[98]\ttraining's binary_logloss: 0.445395\n",
      "[99]\ttraining's binary_logloss: 0.445183\n",
      "[100]\ttraining's binary_logloss: 0.444969\n",
      "Save model...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<lightgbm.basic.Booster at 0x112f28dd8>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params = {\n",
    "    'task': 'train',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'objective': 'binary',\n",
    "    'metric': {'binary_logloss'},\n",
    "    'num_leaves': 32,\n",
    "    'num_trees': 100,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.9,\n",
    "    'bagging_fraction': 0.8,\n",
    "    'bagging_freq': 5,\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "# number of leaves,will be used in feature transformation\n",
    "num_leaf = 64\n",
    "\n",
    "print('Start training...')\n",
    "# train\n",
    "gbm = lgb.train(params,\n",
    "                lgb_train,\n",
    "                num_boost_round=100,\n",
    "                valid_sets=lgb_train)\n",
    "\n",
    "print('Save model...')\n",
    "# save model to file\n",
    "gbm.save_model('lgb-model.txt')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start predicting...\n",
      "(70000, 100)\n",
      "Writing transformed training data\n"
     ]
    }
   ],
   "source": [
    "print('Start predicting...')\n",
    "# predict and get data on leaves, training data\n",
    "y_pred = gbm.predict(X_train_part, pred_leaf=True)\n",
    "\n",
    "print(np.array(y_pred).shape)\n",
    "\n",
    "print('Writing transformed training data')\n",
    "transformed_training_matrix = np.zeros([len(y_pred), len(y_pred[0]) * num_leaf],\n",
    "                                       dtype=np.int64)  # N * num_tress * num_leafs\n",
    "for i in range(0, len(y_pred)):\n",
    "    temp = np.arange(len(y_pred[0])) * num_leaf + np.array(y_pred[i])\n",
    "    transformed_training_matrix[i][temp] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing transformed testing data\n"
     ]
    }
   ],
   "source": [
    "y_pred = gbm.predict(X_valid, pred_leaf=True)\n",
    "print('Writing transformed testing data')\n",
    "transformed_testing_matrix = np.zeros([len(y_pred), len(y_pred[0]) * num_leaf], dtype=np.int64)\n",
    "for i in range(0, len(y_pred)):\n",
    "    temp = np.arange(len(y_pred[0])) * num_leaf + np.array(y_pred[i])\n",
    "    transformed_testing_matrix[i][temp] += 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lm = LogisticRegression(penalty='l2',C=0.05) # logestic model construction\n",
    "lm.fit(transformed_training_matrix,y_train_part)  # fitting the data\n",
    "y_pred_test = lm.predict_proba(transformed_testing_matrix)   # Give the probabilty on each label\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7262627806084931"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lgb_lr_valid_pred = y_pred_test[:,1]\n",
    "\n",
    "roc_auc_score(y_valid, lgb_lr_valid_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalized Cross Entropy 0.8849835781063187\n"
     ]
    }
   ],
   "source": [
    "p=np.sum(y_valid)/len(y_valid)\n",
    "y_valid_eval=np.ones(y_valid.shape)\n",
    "y_valid_eval[y_valid==0]=-1\n",
    "NE = 1/len(y_pred_test) * sum(((1+y_valid_eval)/2 * np.log(y_pred_test[:,1]) +  (1-y_valid_eval)/2 * np.log(1 - y_pred_test[:,1])))/(p*np.log(p)+(1-p)*np.log(1-p))\n",
    "print(\"Normalized Cross Entropy \" + str(NE))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1]\ttraining's binary_logloss: 0.485926\n",
      "[2]\ttraining's binary_logloss: 0.485034\n",
      "[3]\ttraining's binary_logloss: 0.484147\n",
      "[4]\ttraining's binary_logloss: 0.483287\n",
      "[5]\ttraining's binary_logloss: 0.48246\n",
      "[6]\ttraining's binary_logloss: 0.481621\n",
      "[7]\ttraining's binary_logloss: 0.480815\n",
      "[8]\ttraining's binary_logloss: 0.480033\n",
      "[9]\ttraining's binary_logloss: 0.479266\n",
      "[10]\ttraining's binary_logloss: 0.478507\n",
      "[11]\ttraining's binary_logloss: 0.477767\n",
      "[12]\ttraining's binary_logloss: 0.477052\n",
      "[13]\ttraining's binary_logloss: 0.476357\n",
      "[14]\ttraining's binary_logloss: 0.475678\n",
      "[15]\ttraining's binary_logloss: 0.475006\n",
      "[16]\ttraining's binary_logloss: 0.474335\n",
      "[17]\ttraining's binary_logloss: 0.473684\n",
      "[18]\ttraining's binary_logloss: 0.473057\n",
      "[19]\ttraining's binary_logloss: 0.472432\n",
      "[20]\ttraining's binary_logloss: 0.471814\n",
      "[21]\ttraining's binary_logloss: 0.471211\n",
      "[22]\ttraining's binary_logloss: 0.470621\n",
      "[23]\ttraining's binary_logloss: 0.470043\n",
      "[24]\ttraining's binary_logloss: 0.469491\n",
      "[25]\ttraining's binary_logloss: 0.468932\n",
      "[26]\ttraining's binary_logloss: 0.468388\n",
      "[27]\ttraining's binary_logloss: 0.468232\n",
      "[28]\ttraining's binary_logloss: 0.467685\n",
      "[29]\ttraining's binary_logloss: 0.46715\n",
      "[30]\ttraining's binary_logloss: 0.466626\n",
      "[31]\ttraining's binary_logloss: 0.466116\n",
      "[32]\ttraining's binary_logloss: 0.465615\n",
      "[33]\ttraining's binary_logloss: 0.465116\n",
      "[34]\ttraining's binary_logloss: 0.464636\n",
      "[35]\ttraining's binary_logloss: 0.464157\n",
      "[36]\ttraining's binary_logloss: 0.463691\n",
      "[37]\ttraining's binary_logloss: 0.463227\n",
      "[38]\ttraining's binary_logloss: 0.462765\n",
      "[39]\ttraining's binary_logloss: 0.462317\n",
      "[40]\ttraining's binary_logloss: 0.462176\n",
      "[41]\ttraining's binary_logloss: 0.461725\n",
      "[42]\ttraining's binary_logloss: 0.461287\n",
      "[43]\ttraining's binary_logloss: 0.46086\n",
      "[44]\ttraining's binary_logloss: 0.460448\n",
      "[45]\ttraining's binary_logloss: 0.460044\n",
      "[46]\ttraining's binary_logloss: 0.459639\n",
      "[47]\ttraining's binary_logloss: 0.459251\n",
      "[48]\ttraining's binary_logloss: 0.458862\n",
      "[49]\ttraining's binary_logloss: 0.458498\n",
      "[50]\ttraining's binary_logloss: 0.458125\n",
      "[51]\ttraining's binary_logloss: 0.457754\n",
      "[52]\ttraining's binary_logloss: 0.457622\n",
      "[53]\ttraining's binary_logloss: 0.457489\n",
      "[54]\ttraining's binary_logloss: 0.457366\n",
      "[55]\ttraining's binary_logloss: 0.457008\n",
      "[56]\ttraining's binary_logloss: 0.456647\n",
      "[57]\ttraining's binary_logloss: 0.456525\n",
      "[58]\ttraining's binary_logloss: 0.456172\n",
      "[59]\ttraining's binary_logloss: 0.455827\n",
      "[60]\ttraining's binary_logloss: 0.455486\n",
      "[61]\ttraining's binary_logloss: 0.455145\n",
      "[62]\ttraining's binary_logloss: 0.454817\n",
      "[63]\ttraining's binary_logloss: 0.454487\n",
      "[64]\ttraining's binary_logloss: 0.45417\n",
      "[65]\ttraining's binary_logloss: 0.454042\n",
      "[66]\ttraining's binary_logloss: 0.453716\n",
      "[67]\ttraining's binary_logloss: 0.453397\n",
      "[68]\ttraining's binary_logloss: 0.453086\n",
      "[69]\ttraining's binary_logloss: 0.452782\n",
      "[70]\ttraining's binary_logloss: 0.452666\n",
      "[71]\ttraining's binary_logloss: 0.452371\n",
      "[72]\ttraining's binary_logloss: 0.452078\n",
      "[73]\ttraining's binary_logloss: 0.451799\n",
      "[74]\ttraining's binary_logloss: 0.45152\n",
      "[75]\ttraining's binary_logloss: 0.451245\n",
      "[76]\ttraining's binary_logloss: 0.45097\n",
      "[77]\ttraining's binary_logloss: 0.450701\n",
      "[78]\ttraining's binary_logloss: 0.450439\n",
      "[79]\ttraining's binary_logloss: 0.450177\n",
      "[80]\ttraining's binary_logloss: 0.449923\n",
      "[81]\ttraining's binary_logloss: 0.449661\n",
      "[82]\ttraining's binary_logloss: 0.449408\n",
      "[83]\ttraining's binary_logloss: 0.449304\n",
      "[84]\ttraining's binary_logloss: 0.449054\n",
      "[85]\ttraining's binary_logloss: 0.448806\n",
      "[86]\ttraining's binary_logloss: 0.448559\n",
      "[87]\ttraining's binary_logloss: 0.448328\n",
      "[88]\ttraining's binary_logloss: 0.448214\n",
      "[89]\ttraining's binary_logloss: 0.447983\n",
      "[90]\ttraining's binary_logloss: 0.447749\n",
      "[91]\ttraining's binary_logloss: 0.447523\n",
      "[92]\ttraining's binary_logloss: 0.4473\n",
      "[93]\ttraining's binary_logloss: 0.447081\n",
      "[94]\ttraining's binary_logloss: 0.446866\n",
      "[95]\ttraining's binary_logloss: 0.446655\n",
      "[96]\ttraining's binary_logloss: 0.446436\n",
      "[97]\ttraining's binary_logloss: 0.44623\n",
      "[98]\ttraining's binary_logloss: 0.446017\n",
      "[99]\ttraining's binary_logloss: 0.445813\n",
      "[100]\ttraining's binary_logloss: 0.445606\n",
      "Start predicting...\n",
      "(100000, 100)\n",
      "Writing transformed training data\n"
     ]
    }
   ],
   "source": [
    "lgb_full_train = lgb.Dataset(X_train,y_train)\n",
    "gbm_full = lgb.train(params,\n",
    "                lgb_full_train,\n",
    "                num_boost_round=100,\n",
    "                valid_sets=lgb_full_train)\n",
    "print('Start predicting...')\n",
    "# predict and get data on leaves, training data\n",
    "y_pred = gbm_full.predict(X_train, pred_leaf=True)\n",
    "\n",
    "print(np.array(y_pred).shape)\n",
    "\n",
    "print('Writing transformed training data')\n",
    "transformed_training_matrix = np.zeros([len(y_pred), len(y_pred[0]) * num_leaf],\n",
    "                                       dtype=np.int64)  # N * num_tress * num_leafs\n",
    "for i in range(y_pred.shape[0]):\n",
    "    temp=np.arange(y_pred.shape[1])*num_leaf+y_pred[i]\n",
    "    transformed_training_matrix[i,temp]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LogisticRegression(C=1, class_weight=None, dual=False, fit_intercept=True,\n",
       "          intercept_scaling=1, max_iter=100, multi_class='warn',\n",
       "          n_jobs=None, penalty='l2', random_state=None, solver='warn',\n",
       "          tol=0.0001, verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lm = LogisticRegression(penalty='l2',C=1) # logestic model construction\n",
    "lm.fit(transformed_training_matrix,y_train)  # fitting the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_pred = gbm_full.predict(X_test,pred_leaf=True)\n",
    "transformed_testing_matrix=np.zeros((y_pred.shape[0],num_leaf*y_pred.shape[1]))\n",
    "for i in range(y_pred.shape[0]):\n",
    "    temp=np.arange(y_pred.shape[1])*num_leaf+y_pred[i]\n",
    "    transformed_testing_matrix[i,temp]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "lgb_lr_pred_test = lm.predict_proba(transformed_testing_matrix)[:,1]   # Give the probabilty on each label\n",
    "\n",
    "\n",
    "pd.Series(lgb_lr_pred_test, \n",
    "          name='dep_delayed_15min').to_csv('lgb_lr_2feat.csv', \n",
    "                                           index_label='id', header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "xgb_clf = xgb.XGBClassifier(\n",
    "                 colsample_bytree=0.2,\n",
    "                 gamma=0.0,\n",
    "                 learning_rate=0.01,\n",
    "                 max_depth=8,\n",
    "                 min_child_weight=1.5,\n",
    "                 n_estimators=1000,                                                                  \n",
    "                 reg_alpha=0.9,\n",
    "                 reg_lambda=0.6,\n",
    "                 subsample=0.2,\n",
    "                 num\n",
    "                 seed=42,\n",
    "                 silent=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', colsample_bylevel=1,\n",
       "       colsample_bytree=0.2, gamma=0.0, learning_rate=0.01,\n",
       "       max_delta_step=0, max_depth=8, min_child_weight=1.5, missing=None,\n",
       "       n_estimators=1000, n_jobs=1, nthread=None,\n",
       "       objective='binary:logistic', random_state=0, reg_alpha=0.9,\n",
       "       reg_lambda=0.6, scale_pos_weight=1, seed=42, silent=1,\n",
       "       subsample=0.2)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_clf.fit(X_train_part,y_train_part)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7313949358315118"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_valid_pred = xgb_clf.predict_proba(X_valid)[:, 1]\n",
    "\n",
    "roc_auc_score(y_valid, xgb_valid_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
